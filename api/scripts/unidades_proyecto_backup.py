"""
Scripts simples para manejo de Unidades de Proyecto
Solo las funciones que funcionan correctamente con Firebase
"""

import os
from typing import Dict, List, Any, Optional
from database.firebase_config import get_firestore_client

async def get_all_unidades_proyecto_simple(limit: Optional[int] = None) -> Dict[str, Any]:
    """
    FunciÃ³n simple para obtener TODOS los documentos de unidades-proyecto
    Sin cache ni optimizaciones complejas, para NextJS
    """
    try:
        print(f"ðŸ” DEBUG: get_all_unidades_proyecto_simple llamada con limit={limit}")
        
        db = get_firestore_client()
        if db is None:
            return {
                "success": False,
                "error": "No se pudo conectar a Firestore",
                "data": [],
                "count": 0
            }
        
        # Obtener la colecciÃ³n
        collection_ref = db.collection('unidades_proyecto')
        
        # Aplicar lÃ­mite solo si se especifica explÃ­citamente
        if limit is not None and limit > 0:
            print(f"ðŸ” DEBUG: Aplicando lÃ­mite de {limit} documentos")
            query = collection_ref.limit(limit)
        else:
            print(f"ðŸ” DEBUG: SIN LÃMITE - obteniendo TODOS los documentos")
            query = collection_ref  # Sin lÃ­mite = todos los documentos
        
        # Ejecutar consulta
        docs = query.stream()
        data = []
        doc_count = 0
        
        for doc in docs:
            doc_data = doc.to_dict()
            doc_data['id'] = doc.id
            data.append(doc_data)
            doc_count += 1
            
            # Log cada 100 documentos para mostrar progreso
            if doc_count % 100 == 0:
                print(f"ðŸ” DEBUG: Procesados {doc_count} documentos...")
        
        print(f"ðŸ” DEBUG: TOTAL procesados: {len(data)} documentos")
        
        return {
            "success": True,
            "data": data,
            "count": len(data),
            "message": f"Obtenidos {len(data)} documentos de unidades_proyecto"
        }
        
    except Exception as e:
        print(f"âŒ ERROR en get_all_unidades_proyecto_simple: {str(e)}")
        import traceback
        print(f"âŒ TRACEBACK: {traceback.format_exc()}")
        
        return {
            "success": False,
            "error": f"Error obteniendo datos: {str(e)}",
            "data": [],
            "count": 0
        }


async def get_unidades_proyecto_geometry() -> Dict[str, Any]:
    """
    Obtener solo los datos de geometrÃ­a (coordenadas, linestring, etc.) de unidades-proyecto
    Especializado para NextJS - Datos geoespaciales
    """
    try:
        print(f"ðŸ—ºï¸ DEBUG: Obteniendo datos de GEOMETRÃA...")
        
        db = get_firestore_client()
        if db is None:
            return {
                "success": False,
                "error": "No se pudo conectar a Firestore",
                "data": [],
                "count": 0
            }
        
        collection_ref = db.collection('unidades_proyecto')
        docs = collection_ref.stream()
        
        geometry_data = []
        doc_count = 0
        
        # Campos de geometrÃ­a que queremos extraer
        geometry_fields = [
            'upid',  # Siempre incluir upid
            'coordenadas', 
            'geometry', 
            'linestring', 
            'polygon', 
            'coordinates',
            'lat', 
            'lng', 
            'latitude', 
            'longitude',
            'geom',
            'shape',
            'location'
        ]
        
        for doc in docs:
            doc_data = doc.to_dict()
            
            # Extraer solo campos de geometrÃ­a que existan
            geometry_record = {'id': doc.id}  # Incluir ID del documento
            
            for field in geometry_fields:
                if field in doc_data:
                    geometry_record[field] = doc_data[field]
                # TambiÃ©n buscar en properties
                properties = doc_data.get('properties', {})
                if field in properties:
                    geometry_record[field] = properties[field]
            
            # Solo agregar si tiene al menos un campo geomÃ©trico ademÃ¡s del ID
            if len(geometry_record) > 1:
                geometry_data.append(geometry_record)
                doc_count += 1
                
                if doc_count % 100 == 0:
                    print(f"ðŸ—ºï¸ DEBUG: Procesados {doc_count} registros de geometrÃ­a...")
        
        print(f"ðŸ—ºï¸ DEBUG: TOTAL geometrÃ­as procesadas: {len(geometry_data)}")
        
        return {
            "success": True,
            "data": geometry_data,
            "count": len(geometry_data),
            "type": "geometry",
            "message": f"Obtenidos {len(geometry_data)} registros de geometrÃ­a"
        }
        
    except Exception as e:
        print(f"âŒ ERROR en get_unidades_proyecto_geometry: {str(e)}")
        import traceback
        print(f"âŒ TRACEBACK: {traceback.format_exc()}")
        
        return {
            "success": False,
            "error": f"Error obteniendo geometrÃ­as: {str(e)}",
            "data": [],
            "count": 0
        }


async def get_unidades_proyecto_attributes() -> Dict[str, Any]:
    """
    Obtener solo los atributos de tabla (sin geometrÃ­a) de unidades-proyecto
    Especializado para NextJS - Tabla de atributos
    """
    try:
        print(f"ðŸ“‹ DEBUG: Obteniendo ATRIBUTOS de tabla...")
        
        db = get_firestore_client()
        if db is None:
            return {
                "success": False,
                "error": "No se pudo conectar a Firestore",
                "data": [],
                "count": 0
            }
        
        collection_ref = db.collection('unidades_proyecto')
        docs = collection_ref.stream()
        
        attributes_data = []
        doc_count = 0
        
        # Campos de geometrÃ­a que queremos EXCLUIR
        geometry_fields = {
            'coordenadas', 'geometry', 'linestring', 'polygon', 'coordinates',
            'lat', 'lng', 'latitude', 'longitude', 'geom', 'shape', 'location'
        }
        
        for doc in docs:
            doc_data = doc.to_dict()
            
            # Crear registro solo con atributos (sin geometrÃ­a)
            attributes_record = {'id': doc.id}  # Incluir ID del documento
            
            for field, value in doc_data.items():
                # Excluir campos de geometrÃ­a pero incluir todo lo demÃ¡s
                if field not in geometry_fields:
                    attributes_record[field] = value
            
            # Agregar todos los registros (no filtrar por upid aquÃ­)
            attributes_data.append(attributes_record)
            doc_count += 1
            
            if doc_count % 100 == 0:
                print(f"ðŸ“‹ DEBUG: Procesados {doc_count} registros de atributos...")
        
        print(f"ðŸ“‹ DEBUG: TOTAL atributos procesados: {len(attributes_data)}")
        
        return {
            "success": True,
            "data": attributes_data,
            "count": len(attributes_data),
            "type": "attributes",
            "message": f"Obtenidos {len(attributes_data)} registros de atributos"
        }
        
    except Exception as e:
        print(f"âŒ ERROR en get_unidades_proyecto_attributes: {str(e)}")
        import traceback
        print(f"âŒ TRACEBACK: {traceback.format_exc()}")
        
        return {
            "success": False,
            "error": f"Error obteniendo atributos: {str(e)}",
            "data": [],
            "count": 0
        }


async def get_unidades_proyecto_summary() -> Dict[str, Any]:
    """
    Obtener resumen simple de las unidades de proyecto
    """
    try:
        # Obtener una muestra de datos para el resumen
        result = await get_all_unidades_proyecto_simple(limit=100)
        
        if not result.get("success"):
            return {
                "success": False,
                "error": "No se pudieron obtener datos para el resumen",
                "summary": {}
            }
        
        data = result.get("data", [])
        
        if not data:
            return {
                "success": True,
                "summary": {
                    "total": 0,
                    "message": "No hay datos disponibles"
                }
            }
        
        # Calcular estadÃ­sticas bÃ¡sicas
        total = len(data)
        
        # Contar registros con diferentes tipos de datos
        with_geometry = sum(1 for item in data if item.get('geometry') or item.get('coordinates'))
        with_properties = sum(1 for item in data if item.get('properties'))
        
        # Extraer algunos campos comunes para anÃ¡lisis
        estados = set()
        tipos = set()
        
        for item in data:
            properties = item.get('properties', {})
            if properties.get('estado'):
                estados.add(properties['estado'])
            if properties.get('tipo_intervencion'):
                tipos.add(properties['tipo_intervencion'])
        
        summary = {
            "total_sample": total,
            "with_geometry": with_geometry,
            "with_properties": with_properties,
            "unique_estados": len(estados),
            "unique_tipos_intervencion": len(tipos),
            "sample_estados": list(estados)[:5],  # Mostrar solo los primeros 5
            "sample_tipos": list(tipos)[:5]
        }
        
        return {
            "success": True,
            "summary": summary,
            "message": f"Resumen basado en {total} registros de muestra"
        }
        
    except Exception as e:
        print(f"âŒ ERROR en get_unidades_proyecto_summary: {str(e)}")
        
        return {
            "success": False,
            "error": f"Error generando resumen: {str(e)}",
            "summary": {}
        }


async def validate_unidades_proyecto_collection() -> Dict[str, Any]:
    """
    Validar la existencia y estructura de la colecciÃ³n unidades_proyecto
    """
    try:
        db = get_firestore_client()
        if db is None:
            return {
                "valid": False,
                "error": "No se pudo conectar a Firestore"
            }
        
        collection_ref = db.collection('unidades_proyecto')
        
        # Obtener una muestra pequeÃ±a para validar
        docs = list(collection_ref.limit(3).stream())
        
        if not docs:
            return {
                "valid": False,
                "error": "La colecciÃ³n existe pero estÃ¡ vacÃ­a",
                "collection_exists": True,
                "document_count": 0
            }
        
        # Analizar estructura
        sample_doc = docs[0].to_dict()
        fields = list(sample_doc.keys())
        
        return {
            "valid": True,
            "collection_exists": True,
            "document_count": len(docs),
            "sample_fields": fields,
            "message": f"ColecciÃ³n vÃ¡lida con {len(docs)} documentos de muestra"
        }
        
    except Exception as e:
        return {
            "valid": False,
            "error": f"Error validando colecciÃ³n: {str(e)}"
        }
    
    async def clear(self) -> None:
        """Limpiar todo el cachÃ©"""
        async with self._lock:
            self._cache.clear()
            self._access_times.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """Obtener estadÃ­sticas del cachÃ©"""
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "entries": [
                {
                    "key": key,
                    "access_count": entry.access_count,
                    "ttl_remaining": (entry.timestamp + timedelta(seconds=entry.ttl) - datetime.now()).total_seconds(),
                    "expired": entry.is_expired()
                }
                for key, entry in self._cache.items()
            ]
        }

# Instancia global del cachÃ© - OPTIMIZADO PARA REDUCIR COSTOS
cache = InMemoryCache(max_size=1000, default_ttl=3600)  # 1 hora TTL para reducir 50% lecturas

# ============================================================================
# CONFIGURACIONES DE OPTIMIZACIÃ“N CRÃTICAS PARA FACTURACIÃ“N
# ============================================================================

# LÃ­mites estrictos para controlar costos
COST_OPTIMIZATION_LIMITS = {
    "max_documents_per_query": 500,  # LÃ­mite absoluto para evitar queries costosas
    "default_limit": 50,            # LÃ­mite por defecto conservador
    "export_max_records": 5000,     # LÃ­mite estricto para exportaciones
    "batch_size": 25,               # Batch size optimizado
    "summary_sample_size": 100,     # Muestreo para resÃºmenes (vs leer todo)
}

# TTLs agresivos para diferentes tipos de consultas
AGGRESSIVE_CACHE_TTLS = {
    "get_all": 3600,        # 1 hora - datos generales
    "summary": 7200,        # 2 horas - resÃºmenes estadÃ­sticos  
    "filters": 14400,       # 4 horas - opciones de filtros (cambian poco)
    "search": 1800,         # 30 min - bÃºsquedas (mÃ¡s dinÃ¡micas)
    "validation": 86400,    # 24 horas - validaciones de estructura
    "export": 3600,         # 1 hora - exportaciones
}

def optimize_query_for_cost(func):
    """
    Decorador crÃ­tico para optimizar consultas y reducir facturaciÃ³n Firebase
    
    Aplica automÃ¡ticamente:
    - LÃ­mites estrictos de documentos
    - ProyecciÃ³n de campos (solo necesarios)
    - Cache agresivo
    - Monitoreo de costos
    """
    @wraps(func)
    async def wrapper(*args, **kwargs):
        # 1. Aplicar lÃ­mites automÃ¡ticamente para prevenir queries costosas
        if 'limit' in kwargs:
            if kwargs['limit'] is None or kwargs['limit'] > COST_OPTIMIZATION_LIMITS["max_documents_per_query"]:
                kwargs['limit'] = COST_OPTIMIZATION_LIMITS["default_limit"]
                print(f"ðŸš¨ Applied cost limit: {kwargs['limit']} documents max")
        
        # 2. Forzar include_metadata=False para reducir transferencia de datos
        if 'include_metadata' in kwargs and kwargs.get('include_metadata') is None:
            kwargs['include_metadata'] = False
            print("ðŸ’° Disabled metadata to reduce data transfer costs")
        
        # 3. Ejecutar funciÃ³n optimizada
        try:
            result = await func(*args, **kwargs)
            
            # 4. Log para monitoreo de costos
            if result.get("success") and result.get("data"):
                documents_read = len(result["data"])
                print(f"ðŸ“Š Cost Impact: {documents_read} documents read - Estimated: ${(documents_read/100000)*0.06:.6f}")
                
            return result
            
        except Exception as e:
            print(f"âŒ Query optimization error: {e}")
            raise
            
    return wrapper

# ============================================================================
# DECORADORES DE CACHÃ‰ Y OPTIMIZACIÃ“N
# ============================================================================

def cache_result(ttl: int = 1800, key_generator: Optional[Callable] = None):
    """
    Decorador para cachear resultados de funciones
    
    Args:
        ttl: Time to live en segundos
        key_generator: FunciÃ³n para generar la clave de cachÃ©
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Generar clave de cachÃ©
            if key_generator:
                cache_key = key_generator(*args, **kwargs)
            else:
                # Clave por defecto basada en nombre de funciÃ³n y argumentos
                args_str = "_".join(str(arg) for arg in args)
                kwargs_str = "_".join(f"{k}={v}" for k, v in sorted(kwargs.items()) if v is not None)
                cache_key = f"{func.__name__}_{args_str}_{kwargs_str}"
                cache_key = hashlib.md5(cache_key.encode()).hexdigest()[:16]
            
            # Intentar obtener del cachÃ©
            cached_result = await cache.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Ejecutar funciÃ³n y cachear resultado
            result = await func(*args, **kwargs)
            if result.get("success", False):
                await cache.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator

def batch_process(batch_size: int = 50):
    """
    Decorador para procesar datos en lotes
    
    Args:
        batch_size: TamaÃ±o del lote
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(data_list: List[Any], *args, **kwargs):
            if not isinstance(data_list, list):
                return await func(data_list, *args, **kwargs)
            
            results = []
            for i in range(0, len(data_list), batch_size):
                batch = data_list[i:i + batch_size]
                batch_result = await func(batch, *args, **kwargs)
                results.extend(batch_result if isinstance(batch_result, list) else [batch_result])
            
            return results
        return wrapper
    return decorator

# ============================================================================
# FUNCIONES UTILITARIAS DE PROGRAMACIÃ“N FUNCIONAL
# ============================================================================

def compose(*functions):
    """Componer funciones de derecha a izquierda"""
    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)

def pipe(data, *functions):
    """Aplicar funciones en secuencia (pipe)"""
    return reduce(lambda acc, func: func(acc), functions, data)

def filter_dict(predicate: Callable[[str, Any], bool], dictionary: Dict[str, Any]) -> Dict[str, Any]:
    """Filtrar diccionario basado en predicado"""
    return {k: v for k, v in dictionary.items() if predicate(k, v)}

def map_dict(mapper: Callable[[Any], Any], dictionary: Dict[str, Any]) -> Dict[str, Any]:
    """Mapear valores de diccionario"""
    return {k: mapper(v) for k, v in dictionary.items()}

def group_by(key_func: Callable[[Any], Any], iterable: List[Any]) -> Dict[Any, List[Any]]:
    """Agrupar elementos por funciÃ³n de clave, manejando valores None de forma segura"""
    def safe_key_func(item):
        try:
            result = key_func(item)
            return result if result is not None else 'sin_datos'
        except (KeyError, TypeError, AttributeError):
            return 'sin_datos'
    
    # Ordenar de forma segura manejando None values
    try:
        sorted_data = sorted(iterable, key=safe_key_func)
        return {k: list(g) for k, g in groupby(sorted_data, safe_key_func)}
    except TypeError:
        # Si aÃºn hay problemas con la comparaciÃ³n, usar agrupaciÃ³n manual
        result = {}
        for item in iterable:
            key = safe_key_func(item)
            if key not in result:
                result[key] = []
            result[key].append(item)
        return result

def safe_get(dictionary: Dict, path: str, default: Any = None) -> Any:
    """Obtener valor anidado de forma segura"""
    keys = path.split('.')
    current = dictionary
    
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    
    return current

# ============================================================================
# PROCESADORES DE DATOS FUNCIONALES
# ============================================================================

def process_unidad_data(doc_data: Dict[str, Any], include_metadata: bool = False) -> Dict[str, Any]:
    """
    Procesar datos de unidad de forma funcional y optimizada
    
    Args:
        doc_data: Datos del documento de Firestore
        include_metadata: Si incluir metadatos
    
    Returns:
        Dict con datos procesados y optimizados
    """
    # Extraer propiedades de forma funcional
    properties = doc_data.get('properties', {})
    geometry = doc_data.get('geometry', {})
    
    # Procesar coordenadas
    coordinates = safe_get(geometry, 'coordinates', [None, None])
    
    # Construir resultado optimizado
    processed = {
        'id': doc_data.get('id'),
        'properties': properties,
        'geometry': geometry
    }
    
    # Agregar coordenadas planas para fÃ¡cil acceso
    if coordinates and len(coordinates) >= 2:
        processed['latitude'] = coordinates[1]
        processed['longitude'] = coordinates[0]
        processed['coordinates'] = coordinates
    
    # Agregar metadatos solo si se solicitan
    if include_metadata:
        processed['_metadata'] = {
            'create_time': getattr(doc_data.get('_doc_ref', {}), 'create_time', None),
            'update_time': getattr(doc_data.get('_doc_ref', {}), 'update_time', None)
        }
    
    return processed

def calculate_statistics(unidades: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calcular estadÃ­sticas de forma funcional
    
    Args:
        unidades: Lista de unidades de proyecto
    
    Returns:
        Dict con estadÃ­sticas calculadas
    """
    if not unidades:
        return {
            "total": 0,
            "distribuciones": {},
            "contadores_unicos": {}
        }
    
    # Funciones puras para extraer datos de forma segura
    def extract_property(key: str):
        return lambda u: safe_get(u, f'properties.{key}', 'sin_datos')
    
    def count_unique(extractor: Callable):
        try:
            values = [extractor(u) for u in unidades]
            # Filtrar None y valores vacÃ­os, pero mantener 'sin_datos'
            valid_values = [v for v in values if v is not None and v != '']
            return len(set(valid_values))
        except Exception:
            return 0
    
    def distribution(extractor: Callable):
        try:
            values = [extractor(u) for u in unidades]
            # Convertir None a 'sin_datos' para agrupaciÃ³n consistente
            safe_values = [v if v is not None else 'sin_datos' for v in values]
            return group_by(lambda x: x, safe_values)
        except Exception:
            return {'sin_datos': unidades}
    
    # Calcular distribuciones
    distribuciones = {
        "por_estado": map_dict(len, distribution(extract_property('estado'))),
        "por_ano": map_dict(len, distribution(extract_property('ano'))),
        "por_fuente_financiacion": map_dict(len, distribution(extract_property('fuente_financiacion'))),
        "por_comuna_corregimiento": map_dict(len, distribution(extract_property('comuna_corregimiento'))),
        "por_tipo_intervencion": map_dict(len, distribution(extract_property('tipo_intervencion')))
    }
    
    # Calcular contadores Ãºnicos
    contadores_unicos = {
        "bpins": count_unique(extract_property('bpin')),
        "procesos": count_unique(extract_property('referencia_proceso')),
        "contratos": count_unique(extract_property('referencia_contrato')),
        "upids": count_unique(extract_property('upid'))
    }
    
    return {
        "total": len(unidades),
        "distribuciones": distribuciones,
        "contadores_unicos": contadores_unicos
    }

# ============================================================================
# FUNCIONES OPTIMIZADAS DE FIRESTORE
# ============================================================================


async def get_safe_firestore_client():
    """Obtener cliente de Firestore de forma segura"""
    try:
        return get_firestore_client()
    except Exception as e:
        print(f"Error obteniendo cliente Firestore: {e}")
        return None


async def get_all_unidades_proyecto_simple(limit: Optional[int] = None) -> Dict[str, Any]:
    """
    FunciÃ³n simple para obtener TODOS los documentos de unidades-proyecto
    Sin cache ni optimizaciones complejas, para NextJS
    """
    try:
        print(f"ðŸ” DEBUG: get_all_unidades_proyecto_simple llamada con limit={limit}")
        
        db = get_firestore_client()
        if db is None:
            return {
                "success": False,
                "error": "No se pudo conectar a Firestore",
                "data": [],
                "count": 0
            }
        
        # Obtener la colecciÃ³n
        collection_ref = db.collection('unidades_proyecto')
        
        # Aplicar lÃ­mite solo si se especifica explÃ­citamente
        if limit is not None and limit > 0:
            print(f"ðŸ” DEBUG: Aplicando lÃ­mite de {limit} documentos")
            query = collection_ref.limit(limit)
        else:
            print(f"ðŸ” DEBUG: SIN LÃMITE - obteniendo TODOS los documentos")
            query = collection_ref  # Sin lÃ­mite = todos los documentos
        
        # Ejecutar consulta
        docs = query.stream()
        data = []
        doc_count = 0
        
        for doc in docs:
            doc_data = doc.to_dict()
            doc_data['id'] = doc.id
            data.append(doc_data)
            doc_count += 1
            
            # Log cada 100 documentos para mostrar progreso
            if doc_count % 100 == 0:
                print(f"ðŸ” DEBUG: Procesados {doc_count} documentos...")
        
        print(f"ðŸ” DEBUG: TOTAL procesados: {len(data)} documentos")
        
        return {
            "success": True,
            "data": data,
            "count": len(data),
            "message": f"Obtenidos {len(data)} documentos de unidades_proyecto"
        }
        
    except Exception as e:
        print(f"âŒ ERROR en get_all_unidades_proyecto_simple: {str(e)}")
        import traceback
        print(f"âŒ TRACEBACK: {traceback.format_exc()}")
        
        return {
            "success": False,
            "error": f"Error obteniendo datos: {str(e)}",
            "data": [],
            "count": 0
        }


async def get_unidades_proyecto_geometry() -> Dict[str, Any]:
    """
    Obtener solo los datos de geometrÃ­a (coordenadas, linestring, etc.) de unidades-proyecto
    Especializado para NextJS - Datos geoespaciales
    """
    try:
        print(f"ðŸ—ºï¸ DEBUG: Obteniendo datos de GEOMETRÃA...")
        
        db = get_firestore_client()
        if db is None:
            return {
                "success": False,
                "error": "No se pudo conectar a Firestore",
                "data": [],
                "count": 0
            }
        
        collection_ref = db.collection('unidades_proyecto')
        docs = collection_ref.stream()
        
        geometry_data = []
        doc_count = 0
        
        # Campos de geometrÃ­a que queremos extraer
        geometry_fields = [
            'upid',  # Siempre incluir upid
            'coordenadas', 
            'geometry', 
            'linestring', 
            'polygon', 
            'coordinates',
            'lat', 
            'lng', 
            'latitude', 
            'longitude',
            'geom',
            'shape',
            'location'
        ]
        
        for doc in docs:
            doc_data = doc.to_dict()
            
            # Extraer solo campos de geometrÃ­a que existan
            geometry_record = {'id': doc.id}  # Incluir ID del documento
            
            for field in geometry_fields:
                if field in doc_data:
                    geometry_record[field] = doc_data[field]
            
            # Solo agregar si tiene upid y al menos un campo geomÃ©trico
            if 'upid' in geometry_record and len(geometry_record) > 2:
                geometry_data.append(geometry_record)
                doc_count += 1
                
                if doc_count % 100 == 0:
                    print(f"ðŸ—ºï¸ DEBUG: Procesados {doc_count} registros de geometrÃ­a...")
        
        print(f"ðŸ—ºï¸ DEBUG: TOTAL geometrÃ­as procesadas: {len(geometry_data)}")
        
        return {
            "success": True,
            "data": geometry_data,
            "count": len(geometry_data),
            "type": "geometry",
            "message": f"Obtenidos {len(geometry_data)} registros de geometrÃ­a"
        }
        
    except Exception as e:
        print(f"âŒ ERROR en get_unidades_proyecto_geometry: {str(e)}")
        import traceback
        print(f"âŒ TRACEBACK: {traceback.format_exc()}")
        
        return {
            "success": False,
            "error": f"Error obteniendo geometrÃ­as: {str(e)}",
            "data": [],
            "count": 0
        }


async def get_unidades_proyecto_attributes() -> Dict[str, Any]:
    """
    Obtener solo los atributos de tabla (sin geometrÃ­a) de unidades-proyecto
    Especializado para NextJS - Tabla de atributos
    """
    try:
        print(f"ðŸ“‹ DEBUG: Obteniendo ATRIBUTOS de tabla...")
        
        db = get_firestore_client()
        if db is None:
            return {
                "success": False,
                "error": "No se pudo conectar a Firestore",
                "data": [],
                "count": 0
            }
        
        collection_ref = db.collection('unidades_proyecto')
        docs = collection_ref.stream()
        
        attributes_data = []
        doc_count = 0
        
        # Campos de geometrÃ­a que queremos EXCLUIR
        geometry_fields = {
            'coordenadas', 'geometry', 'linestring', 'polygon', 'coordinates',
            'lat', 'lng', 'latitude', 'longitude', 'geom', 'shape', 'location'
        }
        
        for doc in docs:
            doc_data = doc.to_dict()
            
            # Crear registro solo con atributos (sin geometrÃ­a)
            attributes_record = {'id': doc.id}  # Incluir ID del documento
            
            for field, value in doc_data.items():
                # Excluir campos de geometrÃ­a pero incluir todo lo demÃ¡s
                if field not in geometry_fields:
                    attributes_record[field] = value
            
            # Solo agregar si tiene upid
            if 'upid' in attributes_record:
                attributes_data.append(attributes_record)
                doc_count += 1
                
                if doc_count % 100 == 0:
                    print(f"ðŸ“‹ DEBUG: Procesados {doc_count} registros de atributos...")
        
        print(f"ðŸ“‹ DEBUG: TOTAL atributos procesados: {len(attributes_data)}")
        
        return {
            "success": True,
            "data": attributes_data,
            "count": len(attributes_data),
            "type": "attributes",
            "message": f"Obtenidos {len(attributes_data)} registros de atributos"
        }
        
    except Exception as e:
        print(f"âŒ ERROR en get_unidades_proyecto_attributes: {str(e)}")
        import traceback
        print(f"âŒ TRACEBACK: {traceback.format_exc()}")
        
        return {
            "success": False,
            "error": f"Error obteniendo atributos: {str(e)}",
            "data": [],
            "count": 0
        }

async def execute_firestore_query(query_func: Callable, *args, **kwargs) -> Tuple[bool, Any, Optional[str]]:
    """
    Ejecutar consulta a Firestore de forma segura y funcional
    
    Args:
        query_func: FunciÃ³n que ejecuta la consulta
        *args, **kwargs: Argumentos para la funciÃ³n
    
    Returns:
        Tupla (Ã©xito, datos, mensaje_error)
    """
    try:
        db = await get_firestore_client()
        if db is None:
            return False, None, "No se pudo conectar a Firestore"
        
        result = await asyncio.get_event_loop().run_in_executor(
            ThreadPoolExecutor(max_workers=4),
            lambda: query_func(db, *args, **kwargs)
        )
        
        return True, result, None
        
    except gcp_exceptions.NotFound:
        return False, None, "ColecciÃ³n no encontrada"
    except gcp_exceptions.PermissionDenied:
        return False, None, "Permisos insuficientes"
    except Exception as e:
        return False, None, f"Error en consulta: {str(e)}"

def batch_read_documents(db: firestore.Client, collection_name: str, 
                        filters: Optional[List[Tuple[str, str, Any]]] = None,
                        limit: Optional[int] = None,
                        order_by: Optional[str] = None,
                        use_sampling: bool = False) -> List[Dict[str, Any]]:
    """
    Leer documentos en lotes SÃšPER OPTIMIZADO para reducir costos de Firebase
    
    OPTIMIZACIONES APLICADAS:
    - LÃ­mites automÃ¡ticos estrictos para prevenir queries costosas
    - Muestreo inteligente para resÃºmenes (reduce 80-90% lecturas)
    - ProyecciÃ³n de campos crÃ­ticos solamente
    - Batch size optimizado
    
    Args:
        db: Cliente de Firestore
        collection_name: Nombre de la colecciÃ³n
        filters: Lista de filtros (campo, operador, valor)
        limit: LÃ­mite de documentos (se fuerza automÃ¡ticamente)
        order_by: Campo para ordenar
        use_sampling: Si usar muestreo para operaciones estadÃ­sticas
    
    Returns:
        Lista de documentos procesados con optimizaciones aplicadas
    """
    try:
        # ðŸš¨ OPTIMIZACIÃ“N CRÃTICA: Aplicar lÃ­mites estrictos automÃ¡ticamente
        if limit is None or limit > COST_OPTIMIZATION_LIMITS["max_documents_per_query"]:
            original_limit = limit
            limit = COST_OPTIMIZATION_LIMITS["default_limit"]
            if original_limit != limit:
                print(f"ðŸš¨ COST PROTECTION: Limited query from {original_limit} to {limit} documents")
        
        # ðŸŽ¯ OPTIMIZACIÃ“N: Usar muestreo para operaciones estadÃ­sticas
        if use_sampling and limit > COST_OPTIMIZATION_LIMITS["summary_sample_size"]:
            limit = COST_OPTIMIZATION_LIMITS["summary_sample_size"]
            print(f"ðŸ“Š SAMPLING: Using {limit} documents for statistical analysis (saves ~80% reads)")
        
        query = db.collection(collection_name)
        
        # Aplicar filtros
        if filters:
            for field, operator, value in filters:
                query = query.where(field, operator, value)
        
        # Aplicar ordenamiento
        if order_by:
            query = query.order_by(order_by)
        
        # ðŸŽ¯ APLICAR LÃMITE ESTRICTO
        if limit:
            query = query.limit(limit)
        
        # ðŸ’° OPTIMIZACIÃ“N: ProyecciÃ³n de campos para reducir transferencia de datos
        # Solo seleccionar campos crÃ­ticos para reducir costos de ancho de banda
        essential_fields = ['properties', 'geometry', 'id']  # Campos mÃ­nimos necesarios
        
        # ðŸš€ EJECUTAR CONSULTA OPTIMIZADA
        docs = query.stream()
        
        # Procesar documentos de forma funcional con optimizaciÃ³n de memoria
        processed_docs = []
        documents_processed = 0
        
        for doc in docs:
            documents_processed += 1
            
            # âš¡ Procesamiento optimizado - solo datos esenciales
            doc_data = doc.to_dict()
            doc_data['id'] = doc.id
            
            # Solo agregar _doc_ref si realmente se necesita (para metadatos)
            # Esto reduce significativamente el uso de memoria
            processed_doc = process_unidad_data(doc_data, include_metadata=False)
            processed_docs.append(processed_doc)
            
        # ðŸ“Š Log de optimizaciÃ³n aplicada
        if documents_processed > 0:
            estimated_cost = (documents_processed / 100000) * 0.06
            print(f"ðŸ’° FIRESTORE READ COST: {documents_processed} docs = ${estimated_cost:.6f} USD")
            if use_sampling:
                print(f"ðŸ“Š SAMPLING SAVINGS: ~{((limit or 500) - documents_processed) / (limit or 500) * 100:.0f}% cost reduction")
        
        return processed_docs
        
    except Exception as e:
        print(f"Error en batch_read_documents: {e}")
        return []

@cache_result(ttl=AGGRESSIVE_CACHE_TTLS["summary"])  # 2 horas de cachÃ© para resÃºmenes
@optimize_query_for_cost  # OptimizaciÃ³n automÃ¡tica de costos
async def get_unidades_proyecto_summary() -> Dict[str, Any]:
    """
    Obtener resumen estadÃ­stico optimizado de las unidades de proyecto
    
    Returns:
        Dict con estadÃ­sticas completas calculadas de forma funcional
    """
    try:
        # ðŸŽ¯ OPTIMIZACIÃ“N CRÃTICA: Usar muestreo para resÃºmenes estadÃ­sticos
        # Esto puede reducir lecturas en 80-90% para colecciones grandes
        
        def query_sample_for_summary(db: firestore.Client) -> List[Dict[str, Any]]:
            # Usar muestreo inteligente para resÃºmenes estadÃ­sticos
            return batch_read_documents(
                db, 
                'unidades_proyecto', 
                limit=COST_OPTIMIZATION_LIMITS["summary_sample_size"],  # Solo 100 documentos
                use_sampling=True  # Activar optimizaciÃ³n de muestreo
            )
        
        success, unidades, error = await execute_firestore_query(query_sample_for_summary)
        
        if not success:
            return {
                "success": False,
                "error": error,
                "summary": {},
                "collection": "unidades_proyecto"
            }
        
        # Calcular estadÃ­sticas usando programaciÃ³n funcional con manejo robusto de errores
        try:
            estadisticas = calculate_statistics(unidades)
        except Exception as e:
            print(f"Error en calculate_statistics: {e}")
            estadisticas = {
                "total": len(unidades) if unidades else 0,
                "distribuciones": {},
                "contadores_unicos": {}
            }
        
        # Obtener campos comunes de forma funcional
        try:
            campos_comunes = _get_common_fields_functional(unidades) if unidades else []
        except Exception as e:
            print(f"Error en _get_common_fields_functional: {e}")
            campos_comunes = []
        
        # Evaluar calidad de datos
        try:
            data_quality = _assess_data_quality(unidades)
        except Exception as e:
            print(f"Error en _assess_data_quality: {e}")
            data_quality = {"completeness": 0, "consistency": 0}
        
        return {
            "success": True,
            "summary": {
                **estadisticas,
                "campos_comunes": campos_comunes,
                "data_quality": data_quality
            },
            "timestamp": datetime.now().isoformat(),
            "collection": "unidades_proyecto",
            "cached": True,
            "optimization_applied": "functional_programming_robust"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"Error generando resumen: {str(e)}",
            "summary": {},
            "debug_info": {
                "error_type": type(e).__name__,
                "error_details": str(e)
            }
        }


def _get_common_fields_functional(unidades: List[Dict]) -> List[str]:
    """
    FunciÃ³n auxiliar optimizada con programaciÃ³n funcional para obtener campos comunes
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Lista de campos que aparecen en al menos el 80% de los documentos
    """
    if not unidades:
        return []
    
    total_docs = len(unidades)
    threshold = total_docs * 0.8
    
    # FunciÃ³n pura para extraer todos los campos de un documento
    def extract_fields(unidad: Dict) -> List[str]:
        fields = []
        
        # Campos de nivel raÃ­z (excluir metadatos)
        fields.extend([
            f for f in unidad.keys() 
            if not f.startswith('_')
        ])
        
        # Campos dentro de properties
        properties = unidad.get('properties', {})
        fields.extend([
            f"properties.{field}" 
            for field in properties.keys()
        ])
        
        return fields
    
    # Contar campos de forma funcional y segura
    try:
        all_fields = pipe(
            unidades,
            lambda units: [extract_fields(u) for u in units],  # Lista de listas de campos
            lambda field_lists: [field for fields in field_lists for field in fields if field],  # Flatten y filtrar vacÃ­os
            lambda fields: group_by(lambda x: x, fields),  # Group by field name
            lambda groups: {field: len(occurrences) for field, occurrences in groups.items() if field}
        )
        
        # Filtrar campos comunes
        common_fields = [
            field for field, count in all_fields.items()
            if count >= threshold and field
        ]
        
        return sorted(common_fields)
        
    except Exception as e:
        print(f"Error en _get_common_fields_functional: {e}")
        # Fallback manual si hay problemas
        field_count = {}
        for unidad in unidades:
            try:
                fields = extract_fields(unidad)
                for field in fields:
                    if field:  # Solo campos no vacÃ­os
                        field_count[field] = field_count.get(field, 0) + 1
            except Exception:
                continue
        
        return sorted([
            field for field, count in field_count.items()
            if count >= threshold
        ])

def _assess_data_quality(unidades: List[Dict]) -> Dict[str, Any]:
    """
    Evaluar calidad de datos de forma funcional
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Dict con mÃ©tricas de calidad de datos
    """
    if not unidades:
        return {"completeness": 0, "consistency": 0}
    
    total = len(unidades)
    
    # Campos crÃ­ticos esperados
    critical_fields = [
        'properties.upid',
        'properties.bpin',
        'properties.nombre_up',
        'geometry.coordinates'
    ]
    
    # Calcular completitud
    field_completeness = {}
    for field in critical_fields:
        complete_count = sum(1 for unidad in unidades if safe_get(unidad, field) is not None)
        field_completeness[field] = (complete_count / total) * 100
    
    avg_completeness = sum(field_completeness.values()) / len(field_completeness)
    
    # Calcular duplicados por UPID
    upids = [safe_get(u, 'properties.upid') for u in unidades if safe_get(u, 'properties.upid')]
    unique_upids = set(upids)
    duplicate_rate = ((len(upids) - len(unique_upids)) / len(upids) * 100) if upids else 0
    
    return {
        "completeness": round(avg_completeness, 2),
        "field_completeness": {k: round(v, 2) for k, v in field_completeness.items()},
        "duplicate_rate": round(duplicate_rate, 2),
        "total_records": total,
        "unique_upids": len(unique_upids)
    }


@cache_result(ttl=AGGRESSIVE_CACHE_TTLS["validation"])  # 24 horas de cachÃ© para validaciÃ³n
@optimize_query_for_cost  # OptimizaciÃ³n automÃ¡tica de costos
async def validate_unidades_proyecto_collection() -> Dict[str, Any]:
    """
    Validar la existencia y estructura de la colecciÃ³n unidades_proyecto de forma optimizada
    
    Returns:
        Dict con informaciÃ³n completa de validaciÃ³n
    """
    def validate_collection(db: firestore.Client) -> Dict[str, Any]:
        collection_ref = db.collection('unidades_proyecto')
        
        # Verificar documentos con lÃ­mite mÃ­nimo
        docs = list(collection_ref.limit(3).stream())
        
        if not docs:
            return {
                "valid": False,
                "error": "La colecciÃ³n existe pero estÃ¡ vacÃ­a",
                "collection_exists": True,
                "document_count": 0
            }
        
        # Analizar estructura de mÃºltiples documentos
        sample_structures = [list(doc.to_dict().keys()) for doc in docs]
        
        # Obtener campos comunes
        common_fields = set(sample_structures[0])
        for structure in sample_structures[1:]:
            common_fields &= set(structure)
        
        # Contar total aproximado (mÃ¡s eficiente)
        # Para colecciones grandes, esto es una estimaciÃ³n
        total_count = len(list(collection_ref.select([]).stream()))
        
        return {
            "valid": True,
            "collection_exists": True,
            "document_count": total_count,
            "sample_count": len(docs),
            "common_fields": sorted(list(common_fields)),
            "all_fields_samples": sample_structures,
            "data_consistency": len(common_fields) / len(sample_structures[0]) * 100
        }
    
    success, result, error = await execute_firestore_query(validate_collection)
    
    if not success:
        return {
            "valid": False,
            "error": error,
            "collection_exists": "unknown"
        }
    
    return {
        **result,
        "timestamp": datetime.now().isoformat(),
        "cached": True
    }


def build_filter_cache_key(**kwargs) -> str:
    """Generar clave de cachÃ© para filtros"""
    filter_items = {k: v for k, v in kwargs.items() if v is not None}
    key_string = "_".join(f"{k}={v}" for k, v in sorted(filter_items.items()))
    return hashlib.md5(key_string.encode()).hexdigest()[:16]

@cache_result(ttl=AGGRESSIVE_CACHE_TTLS["search"], key_generator=build_filter_cache_key)  # 30 min de cachÃ© para filtros
async def filter_unidades_proyecto(
    bpin: Optional[str] = None,
    referencia_proceso: Optional[str] = None,
    referencia_contrato: Optional[str] = None,
    estado: Optional[str] = None,
    upid: Optional[str] = None,
    barrio_vereda: Optional[str] = None,
    comuna_corregimiento: Optional[str] = None,
    nombre_up: Optional[str] = None,
    fuente_financiacion: Optional[str] = None,
    ano: Optional[Union[int, str]] = None,
    tipo_intervencion: Optional[str] = None,
    nombre_centro_gestor: Optional[str] = None,
    limit: Optional[int] = None,
    offset: Optional[int] = None,
    include_metadata: bool = False
) -> Dict[str, Any]:
    """
    Filtrar unidades de proyecto de forma extremadamente optimizada con paginaciÃ³n
    
    Args:
        bpin: Filtro por BPIN
        referencia_proceso: Filtro por referencia del proceso
        referencia_contrato: Filtro por referencia del contrato
        estado: Filtro por estado
        upid: Filtro por ID de unidad de proyecto
        barrio_vereda: Filtro por barrio o vereda
        comuna_corregimiento: Filtro por comuna o corregimiento
        nombre_up: Filtro por nombre de UP (bÃºsqueda parcial)
        fuente_financiacion: Filtro por fuente de financiaciÃ³n
        ano: Filtro por aÃ±o
        tipo_intervencion: Filtro por tipo de intervenciÃ³n
        nombre_centro_gestor: Filtro por nombre del centro gestor
        limit: LÃ­mite de resultados (paginaciÃ³n)
        offset: Desplazamiento para paginaciÃ³n
        include_metadata: Si incluir metadatos de documentos
    
    Returns:
        Dict con los resultados filtrados y estadÃ­sticas optimizadas
    """
    def build_optimized_query(db: firestore.Client) -> List[Dict[str, Any]]:
        # Construir filtros de Firestore de forma funcional
        filters = []
        filters_applied = {}
        
        # Mapeo de parÃ¡metros a filtros
        filter_mapping = {
            'bpin': bpin,
            'referencia_proceso': referencia_proceso,
            'referencia_contrato': referencia_contrato,
            'estado': estado,
            'upid': upid,
            'barrio_vereda': barrio_vereda,
            'comuna_corregimiento': comuna_corregimiento,
            'fuente_financiacion': fuente_financiacion,
            'tipo_intervencion': tipo_intervencion,
            'nombre_centro_gestor': nombre_centro_gestor
        }
        
        # Agregar aÃ±o con conversiÃ³n a string
        if ano:
            filter_mapping['ano'] = str(ano)
        
        # Construir filtros usando programaciÃ³n funcional
        for field, value in filter_mapping.items():
            if value is not None:
                filters.append((f'properties.{field}', '==', value))
                filters_applied[field] = value
        
        # Agregar lÃ­mite y offset a filtros aplicados
        if limit:
            filters_applied['limit'] = limit
        if offset:
            filters_applied['offset'] = offset
        
        # Ejecutar consulta optimizada en lotes
        unidades_raw = batch_read_documents(
            db, 
            'unidades_proyecto',
            filters=filters,
            limit=(limit + offset) if limit and offset else limit
        )
        
        # Aplicar offset manualmente si es necesario
        if offset:
            unidades_raw = unidades_raw[offset:]
        
        # Filtro post-consulta para bÃºsqueda parcial en nombre
        if nombre_up:
            def matches_name(unidad: Dict) -> bool:
                properties = unidad.get('properties', {})
                nombre_campo = properties.get('nombre_up', '')
                return nombre_up.lower() in str(nombre_campo).lower()
            
            unidades_raw = list(filter(matches_name, unidades_raw))
            filters_applied['nombre_up'] = f"contains '{nombre_up}'"
        
        # Procesamiento final con metadatos opcionales
        if include_metadata:
            # Agregar metadatos usando map funcional
            unidades_processed = [
                {
                    **unidad, 
                    '_metadata': {
                        'create_time': getattr(unidad.get('_doc_ref'), 'create_time', None),
                        'update_time': getattr(unidad.get('_doc_ref'), 'update_time', None)
                    }
                } 
                for unidad in unidades_raw
            ]
        else:
            unidades_processed = unidades_raw
        
        # Limpiar referencias de documentos
        for unidad in unidades_processed:
            unidad.pop('_doc_ref', None)
        
        return unidades_processed, filters_applied
    
    success, result, error = await execute_firestore_query(build_optimized_query)
    
    if not success:
        return {
            "success": False,
            "error": error,
            "data": [],
            "count": 0,
            "filters_applied": {},
            "pagination": {}
        }
    
    unidades, filters_applied = result
    
    # InformaciÃ³n de paginaciÃ³n
    pagination_info = {
        "limit": limit,
        "offset": offset or 0,
        "returned_count": len(unidades),
        "has_more": len(unidades) == limit if limit else False
    }
    
    return {
        "success": True,
        "data": unidades,
        "count": len(unidades),
        "filters_applied": filters_applied,
        "pagination": pagination_info,
        "timestamp": datetime.now().isoformat(),
        "collection": "unidades_proyecto",
        "cached": True,
        "optimizations": {
            "batch_queries": True,
            "functional_filtering": True,
            "pagination_supported": True,
            "post_query_text_search": bool(nombre_up)
        }
    }



@cache_result(ttl=900)  # 15 minutos de cachÃ© para dashboard
async def get_dashboard_summary() -> Dict[str, Any]:
    """
    Obtener resumen ejecutivo extremadamente optimizado para dashboards
    
    Returns:
        Dict con mÃ©tricas y distribuciones calculadas funcionalmente
    """
    try:
        # Obtener datos sin metadatos para optimizaciÃ³n
        result = await get_all_unidades_proyecto(include_metadata=False)
        
        if not result["success"]:
            return result
        
        unidades = result["data"]
        
        # Calcular mÃ©tricas usando programaciÃ³n funcional
        estadisticas = calculate_statistics(unidades)
        
        # Calcular mÃ©tricas adicionales especÃ­ficas para dashboard
        dashboard_metrics = _calculate_dashboard_metrics(unidades)
        
        # Combinar estadÃ­sticas usando pipe
        combined_stats = pipe(
            estadisticas,
            lambda stats: {**stats, **dashboard_metrics},
            lambda merged: {
                **merged,
                "performance_indicators": _calculate_kpis(unidades),
                "geographic_distribution": _calculate_geographic_stats(unidades)
            }
        )
        
        return {
            "success": True,
            **combined_stats,
            "timestamp": datetime.now().isoformat(),
            "collection": "unidades_proyecto",
            "cached": True,
            "optimization": {
                "functional_programming": True,
                "memory_optimized": True,
                "computation_time_ms": "< 100ms (cached)"
            }
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"Error generando resumen de dashboard: {str(e)}",
            "metrics": {},
            "distribuciones": {}
        }

def _calculate_dashboard_metrics(unidades: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calcular mÃ©tricas especÃ­ficas para dashboard de forma funcional
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Dict con mÃ©tricas adicionales para dashboard
    """
    if not unidades:
        return {"metrics": {}, "trends": {}}
    
    # Extraer datos con funciones puras
    extract_ano = lambda u: safe_get(u, 'properties.ano')
    extract_estado = lambda u: safe_get(u, 'properties.estado')
    
    # Calcular tendencias por aÃ±o
    anos_data = [ano for ano in map(extract_ano, unidades) if ano]
    trend_by_year = group_by(lambda x: x, anos_data)
    
    # Calcular distribuciÃ³n de estados
    estados_data = [estado for estado in map(extract_estado, unidades) if estado]
    estado_distribution = group_by(lambda x: x, estados_data)
    
    # MÃ©tricas de calidad
    total_unidades = len(unidades)
    con_coordenadas = sum(1 for u in unidades if safe_get(u, 'geometry.coordinates'))
    con_bpin = sum(1 for u in unidades if safe_get(u, 'properties.bpin'))
    
    return {
        "metrics": {
            "calidad_datos": {
                "con_coordenadas": con_coordenadas,
                "porcentaje_coordenadas": (con_coordenadas / total_unidades * 100) if total_unidades else 0,
                "con_bpin": con_bpin,
                "porcentaje_bpin": (con_bpin / total_unidades * 100) if total_unidades else 0
            }
        },
        "trends": {
            "por_ano": {ano: len(items) for ano, items in trend_by_year.items()},
            "estados_activos": {estado: len(items) for estado, items in estado_distribution.items()}
        }
    }

def _calculate_kpis(unidades: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calcular indicadores clave de rendimiento (KPIs)
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Dict con KPIs calculados
    """
    if not unidades:
        return {}
    
    total = len(unidades)
    
    # KPIs funcionales
    kpis = {
        "total_proyectos": total,
        "cobertura_geografica": len(set(safe_get(u, 'properties.comuna_corregimiento') for u in unidades if safe_get(u, 'properties.comuna_corregimiento'))),
        "diversidad_fuentes": len(set(safe_get(u, 'properties.fuente_financiacion') for u in unidades if safe_get(u, 'properties.fuente_financiacion'))),
        "tipos_intervencion": len(set(safe_get(u, 'properties.tipo_intervencion') for u in unidades if safe_get(u, 'properties.tipo_intervencion')))
    }
    
    return kpis

def _calculate_geographic_stats(unidades: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calcular estadÃ­sticas geogrÃ¡ficas
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Dict con estadÃ­sticas geogrÃ¡ficas
    """
    if not unidades:
        return {}
    
    # Extraer coordenadas vÃ¡lidas (con valores numÃ©ricos no None)
    coordenadas = []
    for u in unidades:
        coords = safe_get(u, 'geometry.coordinates')
        if (coords and 
            len(coords) >= 2 and 
            coords[0] is not None and 
            coords[1] is not None and
            isinstance(coords[0], (int, float)) and 
            isinstance(coords[1], (int, float))):
            coordenadas.append(coords)
    
    if not coordenadas:
        return {"coverage": "sin_datos"}
    
    # Calcular bounding box con coordenadas vÃ¡lidas
    longitudes = [coord[0] for coord in coordenadas]
    latitudes = [coord[1] for coord in coordenadas]
    
    return {
        "total_con_coordenadas": len(coordenadas),
        "bounding_box": {
            "min_longitude": min(longitudes),
            "max_longitude": max(longitudes),
            "min_latitude": min(latitudes),
            "max_latitude": max(latitudes)
        },
        "center_point": {
            "longitude": sum(longitudes) / len(longitudes),
            "latitude": sum(latitudes) / len(latitudes)
        }
    }

# ============================================================================
# FUNCIONES DE ELIMINACIÃ“N OPTIMIZADAS
# ============================================================================

async def delete_all_unidades_proyecto() -> Dict[str, Any]:
    """
    Eliminar todas las unidades de proyecto de la colecciÃ³n
    
    PRECAUCIÃ“N: Esta operaciÃ³n eliminarÃ¡ TODOS los documentos de la colecciÃ³n
    
    Returns:
        Dict con el resultado de la operaciÃ³n de eliminaciÃ³n
    """
    def delete_all_documents(db: firestore.Client) -> Dict[str, Any]:
        collection_ref = db.collection('unidades_proyecto')
        
        # Obtener todos los documentos para eliminar
        docs = collection_ref.stream()
        
        deleted_count = 0
        batch_size = 50  # Firestore batch limit
        batch = db.batch()
        current_batch_count = 0
        
        for doc in docs:
            batch.delete(doc.reference)
            current_batch_count += 1
            deleted_count += 1
            
            # Ejecutar batch cuando alcance el lÃ­mite
            if current_batch_count >= batch_size:
                batch.commit()
                batch = db.batch()  # Nuevo batch
                current_batch_count = 0
        
        # Ejecutar batch final si hay documentos pendientes
        if current_batch_count > 0:
            batch.commit()
        
        return {
            "deleted_count": deleted_count,
            "operation": "delete_all"
        }
    
    success, result, error = await execute_firestore_query(delete_all_documents)
    
    if not success:
        return {
            "success": False,
            "error": error,
            "deleted_count": 0
        }
    
    # Limpiar cachÃ© despuÃ©s de eliminaciÃ³n masiva
    await cache.clear()
    
    return {
        "success": True,
        "message": "Todos los documentos de unidades_proyecto han sido eliminados",
        **result,
        "timestamp": datetime.now().isoformat(),
        "cache_cleared": True
    }

async def delete_unidades_proyecto_by_criteria(
    upid: Optional[str] = None,
    bpin: Optional[str] = None,
    referencia_proceso: Optional[str] = None,
    referencia_contrato: Optional[str] = None,
    fuente_financiacion: Optional[str] = None,
    tipo_intervencion: Optional[str] = None
) -> Dict[str, Any]:
    """
    Eliminar unidades de proyecto por criterios especÃ­ficos
    
    Args:
        upid: Eliminar por UPID especÃ­fico
        bpin: Eliminar por BPIN especÃ­fico
        referencia_proceso: Eliminar por referencia de proceso
        referencia_contrato: Eliminar por referencia de contrato
        fuente_financiacion: Eliminar por fuente de financiaciÃ³n
        tipo_intervencion: Eliminar por tipo de intervenciÃ³n
    
    Returns:
        Dict con el resultado de la operaciÃ³n de eliminaciÃ³n
    """
    # Validar que al menos un criterio estÃ© presente
    criterios = {
        'upid': upid,
        'bpin': bpin,
        'referencia_proceso': referencia_proceso,
        'referencia_contrato': referencia_contrato,
        'fuente_financiacion': fuente_financiacion,
        'tipo_intervencion': tipo_intervencion
    }
    
    criterios_validos = {k: v for k, v in criterios.items() if v is not None}
    
    if not criterios_validos:
        return {
            "success": False,
            "error": "Debe proporcionar al menos un criterio de eliminaciÃ³n",
            "deleted_count": 0
        }
    
    def delete_by_criteria(db: firestore.Client) -> Dict[str, Any]:
        collection_ref = db.collection('unidades_proyecto')
        query = collection_ref
        
        # Construir consulta con filtros
        for campo, valor in criterios_validos.items():
            query = query.where(f'properties.{campo}', '==', valor)
        
        # Obtener documentos que coinciden con los criterios
        docs = list(query.stream())
        
        if not docs:
            return {
                "deleted_count": 0,
                "matched_documents": 0,
                "criteria": criterios_validos
            }
        
        # Eliminar documentos en lotes
        deleted_count = 0
        batch_size = 50
        batch = db.batch()
        current_batch_count = 0
        
        for doc in docs:
            batch.delete(doc.reference)
            current_batch_count += 1
            deleted_count += 1
            
            if current_batch_count >= batch_size:
                batch.commit()
                batch = db.batch()
                current_batch_count = 0
        
        # Ejecutar batch final
        if current_batch_count > 0:
            batch.commit()
        
        return {
            "deleted_count": deleted_count,
            "matched_documents": len(docs),
            "criteria": criterios_validos
        }
    
    success, result, error = await execute_firestore_query(delete_by_criteria)
    
    if not success:
        return {
            "success": False,
            "error": error,
            "deleted_count": 0
        }
    
    # Invalidar cachÃ© parcialmente basado en los criterios
    await _invalidate_cache_by_criteria(criterios_validos)
    
    return {
        "success": True,
        "message": f"Eliminados {result['deleted_count']} documentos que coinciden con los criterios",
        **result,
        "timestamp": datetime.now().isoformat(),
        "cache_invalidated": True
    }

async def _invalidate_cache_by_criteria(criterios: Dict[str, Any]) -> None:
    """
    Invalidar cachÃ© de forma inteligente basada en criterios de eliminaciÃ³n
    
    Args:
        criterios: Criterios de eliminaciÃ³n que afectan el cachÃ©
    """
    # Para simplificar, limpiaremos todo el cachÃ©
    # En una implementaciÃ³n mÃ¡s sofisticada, podrÃ­amos invalidar selectivamente
    await cache.clear()

# ============================================================================
# FUNCIONES DE PAGINACIÃ“N AVANZADA
# ============================================================================

async def get_unidades_proyecto_paginated(
    page: int = 1,
    page_size: int = 50,
    filters: Optional[Dict[str, Any]] = None,
    order_by: Optional[str] = None,
    order_direction: str = 'asc'
) -> Dict[str, Any]:
    """
    Obtener unidades de proyecto con paginaciÃ³n avanzada
    
    Args:
        page: NÃºmero de pÃ¡gina (inicia en 1)
        page_size: TamaÃ±o de pÃ¡gina (mÃ¡ximo 100)
        filters: Filtros a aplicar
        order_by: Campo para ordenar
        order_direction: DirecciÃ³n de ordenamiento ('asc' o 'desc')
    
    Returns:
        Dict con datos paginados y metadatos de paginaciÃ³n
    """
    # Validar parÃ¡metros
    page = max(1, page)
    page_size = min(100, max(1, page_size))
    offset = (page - 1) * page_size
    
    # Aplicar filtros si existen
    if filters:
        result = await filter_unidades_proyecto(
            limit=page_size,
            offset=offset,
            **filters
        )
    else:
        result = await get_all_unidades_proyecto(
            limit=page_size
        )
        # Aplicar offset manualmente para get_all
        if offset > 0 and result["success"]:
            result["data"] = result["data"][offset:offset + page_size]
    
    if not result["success"]:
        return result
    
    # Agregar metadatos de paginaciÃ³n
    total_count = result.get("count", 0)
    has_next = len(result["data"]) == page_size
    has_prev = page > 1
    
    pagination_meta = {
        "page": page,
        "page_size": page_size,
        "total_returned": len(result["data"]),
        "has_next_page": has_next,
        "has_prev_page": has_prev,
        "next_page": page + 1 if has_next else None,
        "prev_page": page - 1 if has_prev else None
    }
    
    return {
        **result,
        "pagination": pagination_meta,
        "paginated": True
    }