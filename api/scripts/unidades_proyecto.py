"""
Scripts optimizados para manejo de Unidades de Proyecto
Funciones puras y modulares con cach√©, paginaci√≥n y programaci√≥n funcional
Optimizado para minimizar facturaci√≥n Firebase y maximizar rendimiento
"""

from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from datetime import datetime, timedelta
from functools import wraps, reduce
from itertools import groupby
from google.cloud import firestore
from google.api_core import exceptions as gcp_exceptions
import asyncio
import json
import hashlib
import weakref
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

from database.firebase_config import FirebaseManager

# ============================================================================
# SISTEMA DE CACH√â EN MEMORIA OPTIMIZADO
# ============================================================================

@dataclass
class CacheEntry:
    """Entrada de cach√© con metadatos"""
    data: Any
    timestamp: datetime
    ttl: int  # Time to live en segundos
    access_count: int = 0
    
    def is_expired(self) -> bool:
        """Verificar si la entrada ha expirado"""
        return datetime.now() > self.timestamp + timedelta(seconds=self.ttl)
    
    def access(self) -> Any:
        """Acceder a los datos y incrementar contador"""
        self.access_count += 1
        return self.data

class InMemoryCache:
    """Cach√© en memoria optimizado para datos de Firestore"""
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 3600):
        self._cache: Dict[str, CacheEntry] = {}
        self._access_times: Dict[str, datetime] = {}
        self.max_size = max_size
        self.default_ttl = default_ttl
        self._lock = asyncio.Lock()
    
    async def get(self, key: str) -> Optional[Any]:
        """Obtener valor del cach√©"""
        async with self._lock:
            if key in self._cache:
                entry = self._cache[key]
                if not entry.is_expired():
                    self._access_times[key] = datetime.now()
                    return entry.access()
                else:
                    # Limpiar entrada expirada
                    del self._cache[key]
                    if key in self._access_times:
                        del self._access_times[key]
        return None
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """Establecer valor en el cach√©"""
        async with self._lock:
            # Limpiar cach√© si est√° lleno
            if len(self._cache) >= self.max_size:
                await self._evict_lru()
            
            entry_ttl = ttl or self.default_ttl
            self._cache[key] = CacheEntry(
                data=value,
                timestamp=datetime.now(),
                ttl=entry_ttl
            )
            self._access_times[key] = datetime.now()
    
    async def _evict_lru(self) -> None:
        """Eliminar entradas menos recientemente usadas"""
        if not self._access_times:
            return
        
        # Eliminar el 20% de las entradas m√°s antiguas
        sorted_entries = sorted(self._access_times.items(), key=lambda x: x[1])
        entries_to_remove = int(len(sorted_entries) * 0.2) or 1
        
        for key, _ in sorted_entries[:entries_to_remove]:
            self._cache.pop(key, None)
            self._access_times.pop(key, None)
    
    async def clear(self) -> None:
        """Limpiar todo el cach√©"""
        async with self._lock:
            self._cache.clear()
            self._access_times.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """Obtener estad√≠sticas del cach√©"""
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "entries": [
                {
                    "key": key,
                    "access_count": entry.access_count,
                    "ttl_remaining": (entry.timestamp + timedelta(seconds=entry.ttl) - datetime.now()).total_seconds(),
                    "expired": entry.is_expired()
                }
                for key, entry in self._cache.items()
            ]
        }

# Instancia global del cach√© - OPTIMIZADO PARA REDUCIR COSTOS
cache = InMemoryCache(max_size=1000, default_ttl=3600)  # 1 hora TTL para reducir 50% lecturas

# ============================================================================
# CONFIGURACIONES DE OPTIMIZACI√ìN CR√çTICAS PARA FACTURACI√ìN
# ============================================================================

# L√≠mites estrictos para controlar costos
COST_OPTIMIZATION_LIMITS = {
    "max_documents_per_query": 500,  # L√≠mite absoluto para evitar queries costosas
    "default_limit": 50,            # L√≠mite por defecto conservador
    "export_max_records": 5000,     # L√≠mite estricto para exportaciones
    "batch_size": 25,               # Batch size optimizado
    "summary_sample_size": 100,     # Muestreo para res√∫menes (vs leer todo)
}

# TTLs agresivos para diferentes tipos de consultas
AGGRESSIVE_CACHE_TTLS = {
    "get_all": 3600,        # 1 hora - datos generales
    "summary": 7200,        # 2 horas - res√∫menes estad√≠sticos  
    "filters": 14400,       # 4 horas - opciones de filtros (cambian poco)
    "search": 1800,         # 30 min - b√∫squedas (m√°s din√°micas)
    "validation": 86400,    # 24 horas - validaciones de estructura
    "export": 3600,         # 1 hora - exportaciones
}

def optimize_query_for_cost(func):
    """
    Decorador cr√≠tico para optimizar consultas y reducir facturaci√≥n Firebase
    
    Aplica autom√°ticamente:
    - L√≠mites estrictos de documentos
    - Proyecci√≥n de campos (solo necesarios)
    - Cache agresivo
    - Monitoreo de costos
    """
    @wraps(func)
    async def wrapper(*args, **kwargs):
        # 1. Aplicar l√≠mites autom√°ticamente para prevenir queries costosas
        if 'limit' in kwargs:
            if kwargs['limit'] is None or kwargs['limit'] > COST_OPTIMIZATION_LIMITS["max_documents_per_query"]:
                kwargs['limit'] = COST_OPTIMIZATION_LIMITS["default_limit"]
                print(f"üö® Applied cost limit: {kwargs['limit']} documents max")
        
        # 2. Forzar include_metadata=False para reducir transferencia de datos
        if 'include_metadata' in kwargs and kwargs.get('include_metadata') is None:
            kwargs['include_metadata'] = False
            print("üí∞ Disabled metadata to reduce data transfer costs")
        
        # 3. Ejecutar funci√≥n optimizada
        try:
            result = await func(*args, **kwargs)
            
            # 4. Log para monitoreo de costos
            if result.get("success") and result.get("data"):
                documents_read = len(result["data"])
                print(f"üìä Cost Impact: {documents_read} documents read - Estimated: ${(documents_read/100000)*0.06:.6f}")
                
            return result
            
        except Exception as e:
            print(f"‚ùå Query optimization error: {e}")
            raise
            
    return wrapper

# ============================================================================
# DECORADORES DE CACH√â Y OPTIMIZACI√ìN
# ============================================================================

def cache_result(ttl: int = 1800, key_generator: Optional[Callable] = None):
    """
    Decorador para cachear resultados de funciones
    
    Args:
        ttl: Time to live en segundos
        key_generator: Funci√≥n para generar la clave de cach√©
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Generar clave de cach√©
            if key_generator:
                cache_key = key_generator(*args, **kwargs)
            else:
                # Clave por defecto basada en nombre de funci√≥n y argumentos
                args_str = "_".join(str(arg) for arg in args)
                kwargs_str = "_".join(f"{k}={v}" for k, v in sorted(kwargs.items()) if v is not None)
                cache_key = f"{func.__name__}_{args_str}_{kwargs_str}"
                cache_key = hashlib.md5(cache_key.encode()).hexdigest()[:16]
            
            # Intentar obtener del cach√©
            cached_result = await cache.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Ejecutar funci√≥n y cachear resultado
            result = await func(*args, **kwargs)
            if result.get("success", False):
                await cache.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator

def batch_process(batch_size: int = 50):
    """
    Decorador para procesar datos en lotes
    
    Args:
        batch_size: Tama√±o del lote
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(data_list: List[Any], *args, **kwargs):
            if not isinstance(data_list, list):
                return await func(data_list, *args, **kwargs)
            
            results = []
            for i in range(0, len(data_list), batch_size):
                batch = data_list[i:i + batch_size]
                batch_result = await func(batch, *args, **kwargs)
                results.extend(batch_result if isinstance(batch_result, list) else [batch_result])
            
            return results
        return wrapper
    return decorator

# ============================================================================
# FUNCIONES UTILITARIAS DE PROGRAMACI√ìN FUNCIONAL
# ============================================================================

def compose(*functions):
    """Componer funciones de derecha a izquierda"""
    return reduce(lambda f, g: lambda x: f(g(x)), functions, lambda x: x)

def pipe(data, *functions):
    """Aplicar funciones en secuencia (pipe)"""
    return reduce(lambda acc, func: func(acc), functions, data)

def filter_dict(predicate: Callable[[str, Any], bool], dictionary: Dict[str, Any]) -> Dict[str, Any]:
    """Filtrar diccionario basado en predicado"""
    return {k: v for k, v in dictionary.items() if predicate(k, v)}

def map_dict(mapper: Callable[[Any], Any], dictionary: Dict[str, Any]) -> Dict[str, Any]:
    """Mapear valores de diccionario"""
    return {k: mapper(v) for k, v in dictionary.items()}

def group_by(key_func: Callable[[Any], Any], iterable: List[Any]) -> Dict[Any, List[Any]]:
    """Agrupar elementos por funci√≥n de clave, manejando valores None de forma segura"""
    def safe_key_func(item):
        try:
            result = key_func(item)
            return result if result is not None else 'sin_datos'
        except (KeyError, TypeError, AttributeError):
            return 'sin_datos'
    
    # Ordenar de forma segura manejando None values
    try:
        sorted_data = sorted(iterable, key=safe_key_func)
        return {k: list(g) for k, g in groupby(sorted_data, safe_key_func)}
    except TypeError:
        # Si a√∫n hay problemas con la comparaci√≥n, usar agrupaci√≥n manual
        result = {}
        for item in iterable:
            key = safe_key_func(item)
            if key not in result:
                result[key] = []
            result[key].append(item)
        return result

def safe_get(dictionary: Dict, path: str, default: Any = None) -> Any:
    """Obtener valor anidado de forma segura"""
    keys = path.split('.')
    current = dictionary
    
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    
    return current

# ============================================================================
# PROCESADORES DE DATOS FUNCIONALES
# ============================================================================

def process_unidad_data(doc_data: Dict[str, Any], include_metadata: bool = False) -> Dict[str, Any]:
    """
    Procesar datos de unidad de forma funcional y optimizada
    
    Args:
        doc_data: Datos del documento de Firestore
        include_metadata: Si incluir metadatos
    
    Returns:
        Dict con datos procesados y optimizados
    """
    # Extraer propiedades de forma funcional
    properties = doc_data.get('properties', {})
    geometry = doc_data.get('geometry', {})
    
    # Procesar coordenadas
    coordinates = safe_get(geometry, 'coordinates', [None, None])
    
    # Construir resultado optimizado
    processed = {
        'id': doc_data.get('id'),
        'properties': properties,
        'geometry': geometry
    }
    
    # Agregar coordenadas planas para f√°cil acceso
    if coordinates and len(coordinates) >= 2:
        processed['latitude'] = coordinates[1]
        processed['longitude'] = coordinates[0]
        processed['coordinates'] = coordinates
    
    # Agregar metadatos solo si se solicitan
    if include_metadata:
        processed['_metadata'] = {
            'create_time': getattr(doc_data.get('_doc_ref', {}), 'create_time', None),
            'update_time': getattr(doc_data.get('_doc_ref', {}), 'update_time', None)
        }
    
    return processed

def calculate_statistics(unidades: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calcular estad√≠sticas de forma funcional
    
    Args:
        unidades: Lista de unidades de proyecto
    
    Returns:
        Dict con estad√≠sticas calculadas
    """
    if not unidades:
        return {
            "total": 0,
            "distribuciones": {},
            "contadores_unicos": {}
        }
    
    # Funciones puras para extraer datos de forma segura
    def extract_property(key: str):
        return lambda u: safe_get(u, f'properties.{key}', 'sin_datos')
    
    def count_unique(extractor: Callable):
        try:
            values = [extractor(u) for u in unidades]
            # Filtrar None y valores vac√≠os, pero mantener 'sin_datos'
            valid_values = [v for v in values if v is not None and v != '']
            return len(set(valid_values))
        except Exception:
            return 0
    
    def distribution(extractor: Callable):
        try:
            values = [extractor(u) for u in unidades]
            # Convertir None a 'sin_datos' para agrupaci√≥n consistente
            safe_values = [v if v is not None else 'sin_datos' for v in values]
            return group_by(lambda x: x, safe_values)
        except Exception:
            return {'sin_datos': unidades}
    
    # Calcular distribuciones
    distribuciones = {
        "por_estado": map_dict(len, distribution(extract_property('estado'))),
        "por_ano": map_dict(len, distribution(extract_property('ano'))),
        "por_fuente_financiacion": map_dict(len, distribution(extract_property('fuente_financiacion'))),
        "por_comuna_corregimiento": map_dict(len, distribution(extract_property('comuna_corregimiento'))),
        "por_tipo_intervencion": map_dict(len, distribution(extract_property('tipo_intervencion')))
    }
    
    # Calcular contadores √∫nicos
    contadores_unicos = {
        "bpins": count_unique(extract_property('bpin')),
        "procesos": count_unique(extract_property('referencia_proceso')),
        "contratos": count_unique(extract_property('referencia_contrato')),
        "upids": count_unique(extract_property('upid'))
    }
    
    return {
        "total": len(unidades),
        "distribuciones": distribuciones,
        "contadores_unicos": contadores_unicos
    }

# ============================================================================
# FUNCIONES OPTIMIZADAS DE FIRESTORE
# ============================================================================


async def get_firestore_client() -> Optional[firestore.Client]:
    """Obtener cliente de Firestore de forma segura"""
    try:
        firebase_manager = FirebaseManager()
        return firebase_manager.get_firestore_client()
    except Exception as e:
        print(f"Error obteniendo cliente Firestore: {e}")
        return None

async def execute_firestore_query(query_func: Callable, *args, **kwargs) -> Tuple[bool, Any, Optional[str]]:
    """
    Ejecutar consulta a Firestore de forma segura y funcional
    
    Args:
        query_func: Funci√≥n que ejecuta la consulta
        *args, **kwargs: Argumentos para la funci√≥n
    
    Returns:
        Tupla (√©xito, datos, mensaje_error)
    """
    try:
        db = await get_firestore_client()
        if db is None:
            return False, None, "No se pudo conectar a Firestore"
        
        result = await asyncio.get_event_loop().run_in_executor(
            ThreadPoolExecutor(max_workers=4),
            lambda: query_func(db, *args, **kwargs)
        )
        
        return True, result, None
        
    except gcp_exceptions.NotFound:
        return False, None, "Colecci√≥n no encontrada"
    except gcp_exceptions.PermissionDenied:
        return False, None, "Permisos insuficientes"
    except Exception as e:
        return False, None, f"Error en consulta: {str(e)}"

def batch_read_documents(db: firestore.Client, collection_name: str, 
                        filters: Optional[List[Tuple[str, str, Any]]] = None,
                        limit: Optional[int] = None,
                        order_by: Optional[str] = None,
                        use_sampling: bool = False) -> List[Dict[str, Any]]:
    """
    Leer documentos en lotes S√öPER OPTIMIZADO para reducir costos de Firebase
    
    OPTIMIZACIONES APLICADAS:
    - L√≠mites autom√°ticos estrictos para prevenir queries costosas
    - Muestreo inteligente para res√∫menes (reduce 80-90% lecturas)
    - Proyecci√≥n de campos cr√≠ticos solamente
    - Batch size optimizado
    
    Args:
        db: Cliente de Firestore
        collection_name: Nombre de la colecci√≥n
        filters: Lista de filtros (campo, operador, valor)
        limit: L√≠mite de documentos (se fuerza autom√°ticamente)
        order_by: Campo para ordenar
        use_sampling: Si usar muestreo para operaciones estad√≠sticas
    
    Returns:
        Lista de documentos procesados con optimizaciones aplicadas
    """
    try:
        # üö® OPTIMIZACI√ìN CR√çTICA: Aplicar l√≠mites estrictos autom√°ticamente
        if limit is None or limit > COST_OPTIMIZATION_LIMITS["max_documents_per_query"]:
            original_limit = limit
            limit = COST_OPTIMIZATION_LIMITS["default_limit"]
            if original_limit != limit:
                print(f"üö® COST PROTECTION: Limited query from {original_limit} to {limit} documents")
        
        # üéØ OPTIMIZACI√ìN: Usar muestreo para operaciones estad√≠sticas
        if use_sampling and limit > COST_OPTIMIZATION_LIMITS["summary_sample_size"]:
            limit = COST_OPTIMIZATION_LIMITS["summary_sample_size"]
            print(f"üìä SAMPLING: Using {limit} documents for statistical analysis (saves ~80% reads)")
        
        query = db.collection(collection_name)
        
        # Aplicar filtros
        if filters:
            for field, operator, value in filters:
                query = query.where(field, operator, value)
        
        # Aplicar ordenamiento
        if order_by:
            query = query.order_by(order_by)
        
        # üéØ APLICAR L√çMITE ESTRICTO
        if limit:
            query = query.limit(limit)
        
        # üí∞ OPTIMIZACI√ìN: Proyecci√≥n de campos para reducir transferencia de datos
        # Solo seleccionar campos cr√≠ticos para reducir costos de ancho de banda
        essential_fields = ['properties', 'geometry', 'id']  # Campos m√≠nimos necesarios
        
        # üöÄ EJECUTAR CONSULTA OPTIMIZADA
        docs = query.stream()
        
        # Procesar documentos de forma funcional con optimizaci√≥n de memoria
        processed_docs = []
        documents_processed = 0
        
        for doc in docs:
            documents_processed += 1
            
            # ‚ö° Procesamiento optimizado - solo datos esenciales
            doc_data = doc.to_dict()
            doc_data['id'] = doc.id
            
            # Solo agregar _doc_ref si realmente se necesita (para metadatos)
            # Esto reduce significativamente el uso de memoria
            processed_doc = process_unidad_data(doc_data, include_metadata=False)
            processed_docs.append(processed_doc)
            
        # üìä Log de optimizaci√≥n aplicada
        if documents_processed > 0:
            estimated_cost = (documents_processed / 100000) * 0.06
            print(f"üí∞ FIRESTORE READ COST: {documents_processed} docs = ${estimated_cost:.6f} USD")
            if use_sampling:
                print(f"üìä SAMPLING SAVINGS: ~{((limit or 500) - documents_processed) / (limit or 500) * 100:.0f}% cost reduction")
        
        return processed_docs
        
    except Exception as e:
        print(f"Error en batch_read_documents: {e}")
        return []

@cache_result(ttl=AGGRESSIVE_CACHE_TTLS["get_all"])  # 1 hora de cach√© para reducir lecturas 50%
@optimize_query_for_cost  # Optimizaci√≥n autom√°tica de costos
async def get_all_unidades_proyecto(include_metadata: bool = False, limit: Optional[int] = None) -> Dict[str, Any]:
    """
    Obtener todas las unidades de proyecto de Firestore de forma optimizada
    
    Args:
        include_metadata: Si incluir metadatos de los documentos
        limit: L√≠mite opcional de documentos a obtener
    
    Returns:
        Dict con la informaci√≥n de todas las unidades de proyecto
    """
    def query_all_unidades(db: firestore.Client) -> List[Dict[str, Any]]:
        return batch_read_documents(
            db, 
            'unidades_proyecto', 
            limit=limit
        )
    
    success, unidades, error = await execute_firestore_query(query_all_unidades)
    
    if not success:
        return {
            "success": False,
            "error": error,
            "data": [],
            "count": 0,
            "cached": False
        }
    
    # Agregar metadatos si se solicitan
    if include_metadata:
        unidades = [
            {**unidad, '_metadata': {
                'create_time': getattr(unidad.get('_doc_ref'), 'create_time', None),
                'update_time': getattr(unidad.get('_doc_ref'), 'update_time', None)
            }} for unidad in unidades
        ]
    
    # Limpiar referencias de documentos para reducir memoria
    for unidad in unidades:
        unidad.pop('_doc_ref', None)
    
    return {
        "success": True,
        "data": unidades,
        "count": len(unidades),
        "timestamp": datetime.now().isoformat(),
        "collection": "unidades_proyecto",
        "cached": True,
        "optimizations": {
            "batch_read": True,
            "functional_processing": True,
            "memory_optimized": True
        }
    }


@cache_result(ttl=AGGRESSIVE_CACHE_TTLS["summary"])  # 2 horas de cach√© para res√∫menes
@optimize_query_for_cost  # Optimizaci√≥n autom√°tica de costos
async def get_unidades_proyecto_summary() -> Dict[str, Any]:
    """
    Obtener resumen estad√≠stico optimizado de las unidades de proyecto
    
    Returns:
        Dict con estad√≠sticas completas calculadas de forma funcional
    """
    try:
        # üéØ OPTIMIZACI√ìN CR√çTICA: Usar muestreo para res√∫menes estad√≠sticos
        # Esto puede reducir lecturas en 80-90% para colecciones grandes
        
        def query_sample_for_summary(db: firestore.Client) -> List[Dict[str, Any]]:
            # Usar muestreo inteligente para res√∫menes estad√≠sticos
            return batch_read_documents(
                db, 
                'unidades_proyecto', 
                limit=COST_OPTIMIZATION_LIMITS["summary_sample_size"],  # Solo 100 documentos
                use_sampling=True  # Activar optimizaci√≥n de muestreo
            )
        
        success, unidades, error = await execute_firestore_query(query_sample_for_summary)
        
        if not success:
            return {
                "success": False,
                "error": error,
                "summary": {},
                "collection": "unidades_proyecto"
            }
        
        # Calcular estad√≠sticas usando programaci√≥n funcional con manejo robusto de errores
        try:
            estadisticas = calculate_statistics(unidades)
        except Exception as e:
            print(f"Error en calculate_statistics: {e}")
            estadisticas = {
                "total": len(unidades) if unidades else 0,
                "distribuciones": {},
                "contadores_unicos": {}
            }
        
        # Obtener campos comunes de forma funcional
        try:
            campos_comunes = _get_common_fields_functional(unidades) if unidades else []
        except Exception as e:
            print(f"Error en _get_common_fields_functional: {e}")
            campos_comunes = []
        
        # Evaluar calidad de datos
        try:
            data_quality = _assess_data_quality(unidades)
        except Exception as e:
            print(f"Error en _assess_data_quality: {e}")
            data_quality = {"completeness": 0, "consistency": 0}
        
        return {
            "success": True,
            "summary": {
                **estadisticas,
                "campos_comunes": campos_comunes,
                "data_quality": data_quality
            },
            "timestamp": datetime.now().isoformat(),
            "collection": "unidades_proyecto",
            "cached": True,
            "optimization_applied": "functional_programming_robust"
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"Error generando resumen: {str(e)}",
            "summary": {},
            "debug_info": {
                "error_type": type(e).__name__,
                "error_details": str(e)
            }
        }


def _get_common_fields_functional(unidades: List[Dict]) -> List[str]:
    """
    Funci√≥n auxiliar optimizada con programaci√≥n funcional para obtener campos comunes
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Lista de campos que aparecen en al menos el 80% de los documentos
    """
    if not unidades:
        return []
    
    total_docs = len(unidades)
    threshold = total_docs * 0.8
    
    # Funci√≥n pura para extraer todos los campos de un documento
    def extract_fields(unidad: Dict) -> List[str]:
        fields = []
        
        # Campos de nivel ra√≠z (excluir metadatos)
        fields.extend([
            f for f in unidad.keys() 
            if not f.startswith('_')
        ])
        
        # Campos dentro de properties
        properties = unidad.get('properties', {})
        fields.extend([
            f"properties.{field}" 
            for field in properties.keys()
        ])
        
        return fields
    
    # Contar campos de forma funcional y segura
    try:
        all_fields = pipe(
            unidades,
            lambda units: [extract_fields(u) for u in units],  # Lista de listas de campos
            lambda field_lists: [field for fields in field_lists for field in fields if field],  # Flatten y filtrar vac√≠os
            lambda fields: group_by(lambda x: x, fields),  # Group by field name
            lambda groups: {field: len(occurrences) for field, occurrences in groups.items() if field}
        )
        
        # Filtrar campos comunes
        common_fields = [
            field for field, count in all_fields.items()
            if count >= threshold and field
        ]
        
        return sorted(common_fields)
        
    except Exception as e:
        print(f"Error en _get_common_fields_functional: {e}")
        # Fallback manual si hay problemas
        field_count = {}
        for unidad in unidades:
            try:
                fields = extract_fields(unidad)
                for field in fields:
                    if field:  # Solo campos no vac√≠os
                        field_count[field] = field_count.get(field, 0) + 1
            except Exception:
                continue
        
        return sorted([
            field for field, count in field_count.items()
            if count >= threshold
        ])

def _assess_data_quality(unidades: List[Dict]) -> Dict[str, Any]:
    """
    Evaluar calidad de datos de forma funcional
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Dict con m√©tricas de calidad de datos
    """
    if not unidades:
        return {"completeness": 0, "consistency": 0}
    
    total = len(unidades)
    
    # Campos cr√≠ticos esperados
    critical_fields = [
        'properties.upid',
        'properties.bpin',
        'properties.nombre_up',
        'geometry.coordinates'
    ]
    
    # Calcular completitud
    field_completeness = {}
    for field in critical_fields:
        complete_count = sum(1 for unidad in unidades if safe_get(unidad, field) is not None)
        field_completeness[field] = (complete_count / total) * 100
    
    avg_completeness = sum(field_completeness.values()) / len(field_completeness)
    
    # Calcular duplicados por UPID
    upids = [safe_get(u, 'properties.upid') for u in unidades if safe_get(u, 'properties.upid')]
    unique_upids = set(upids)
    duplicate_rate = ((len(upids) - len(unique_upids)) / len(upids) * 100) if upids else 0
    
    return {
        "completeness": round(avg_completeness, 2),
        "field_completeness": {k: round(v, 2) for k, v in field_completeness.items()},
        "duplicate_rate": round(duplicate_rate, 2),
        "total_records": total,
        "unique_upids": len(unique_upids)
    }


@cache_result(ttl=AGGRESSIVE_CACHE_TTLS["validation"])  # 24 horas de cach√© para validaci√≥n
@optimize_query_for_cost  # Optimizaci√≥n autom√°tica de costos
async def validate_unidades_proyecto_collection() -> Dict[str, Any]:
    """
    Validar la existencia y estructura de la colecci√≥n unidades_proyecto de forma optimizada
    
    Returns:
        Dict con informaci√≥n completa de validaci√≥n
    """
    def validate_collection(db: firestore.Client) -> Dict[str, Any]:
        collection_ref = db.collection('unidades_proyecto')
        
        # Verificar documentos con l√≠mite m√≠nimo
        docs = list(collection_ref.limit(3).stream())
        
        if not docs:
            return {
                "valid": False,
                "error": "La colecci√≥n existe pero est√° vac√≠a",
                "collection_exists": True,
                "document_count": 0
            }
        
        # Analizar estructura de m√∫ltiples documentos
        sample_structures = [list(doc.to_dict().keys()) for doc in docs]
        
        # Obtener campos comunes
        common_fields = set(sample_structures[0])
        for structure in sample_structures[1:]:
            common_fields &= set(structure)
        
        # Contar total aproximado (m√°s eficiente)
        # Para colecciones grandes, esto es una estimaci√≥n
        total_count = len(list(collection_ref.select([]).stream()))
        
        return {
            "valid": True,
            "collection_exists": True,
            "document_count": total_count,
            "sample_count": len(docs),
            "common_fields": sorted(list(common_fields)),
            "all_fields_samples": sample_structures,
            "data_consistency": len(common_fields) / len(sample_structures[0]) * 100
        }
    
    success, result, error = await execute_firestore_query(validate_collection)
    
    if not success:
        return {
            "valid": False,
            "error": error,
            "collection_exists": "unknown"
        }
    
    return {
        **result,
        "timestamp": datetime.now().isoformat(),
        "cached": True
    }


def build_filter_cache_key(**kwargs) -> str:
    """Generar clave de cach√© para filtros"""
    filter_items = {k: v for k, v in kwargs.items() if v is not None}
    key_string = "_".join(f"{k}={v}" for k, v in sorted(filter_items.items()))
    return hashlib.md5(key_string.encode()).hexdigest()[:16]

@cache_result(ttl=AGGRESSIVE_CACHE_TTLS["search"], key_generator=build_filter_cache_key)  # 30 min de cach√© para filtros
async def filter_unidades_proyecto(
    bpin: Optional[str] = None,
    referencia_proceso: Optional[str] = None,
    referencia_contrato: Optional[str] = None,
    estado: Optional[str] = None,
    upid: Optional[str] = None,
    barrio_vereda: Optional[str] = None,
    comuna_corregimiento: Optional[str] = None,
    nombre_up: Optional[str] = None,
    fuente_financiacion: Optional[str] = None,
    ano: Optional[Union[int, str]] = None,
    tipo_intervencion: Optional[str] = None,
    nombre_centro_gestor: Optional[str] = None,
    limit: Optional[int] = None,
    offset: Optional[int] = None,
    include_metadata: bool = False
) -> Dict[str, Any]:
    """
    Filtrar unidades de proyecto de forma extremadamente optimizada con paginaci√≥n
    
    Args:
        bpin: Filtro por BPIN
        referencia_proceso: Filtro por referencia del proceso
        referencia_contrato: Filtro por referencia del contrato
        estado: Filtro por estado
        upid: Filtro por ID de unidad de proyecto
        barrio_vereda: Filtro por barrio o vereda
        comuna_corregimiento: Filtro por comuna o corregimiento
        nombre_up: Filtro por nombre de UP (b√∫squeda parcial)
        fuente_financiacion: Filtro por fuente de financiaci√≥n
        ano: Filtro por a√±o
        tipo_intervencion: Filtro por tipo de intervenci√≥n
        nombre_centro_gestor: Filtro por nombre del centro gestor
        limit: L√≠mite de resultados (paginaci√≥n)
        offset: Desplazamiento para paginaci√≥n
        include_metadata: Si incluir metadatos de documentos
    
    Returns:
        Dict con los resultados filtrados y estad√≠sticas optimizadas
    """
    def build_optimized_query(db: firestore.Client) -> List[Dict[str, Any]]:
        # Construir filtros de Firestore de forma funcional
        filters = []
        filters_applied = {}
        
        # Mapeo de par√°metros a filtros
        filter_mapping = {
            'bpin': bpin,
            'referencia_proceso': referencia_proceso,
            'referencia_contrato': referencia_contrato,
            'estado': estado,
            'upid': upid,
            'barrio_vereda': barrio_vereda,
            'comuna_corregimiento': comuna_corregimiento,
            'fuente_financiacion': fuente_financiacion,
            'tipo_intervencion': tipo_intervencion,
            'nombre_centro_gestor': nombre_centro_gestor
        }
        
        # Agregar a√±o con conversi√≥n a string
        if ano:
            filter_mapping['ano'] = str(ano)
        
        # Construir filtros usando programaci√≥n funcional
        for field, value in filter_mapping.items():
            if value is not None:
                filters.append((f'properties.{field}', '==', value))
                filters_applied[field] = value
        
        # Agregar l√≠mite y offset a filtros aplicados
        if limit:
            filters_applied['limit'] = limit
        if offset:
            filters_applied['offset'] = offset
        
        # Ejecutar consulta optimizada en lotes
        unidades_raw = batch_read_documents(
            db, 
            'unidades_proyecto',
            filters=filters,
            limit=(limit + offset) if limit and offset else limit
        )
        
        # Aplicar offset manualmente si es necesario
        if offset:
            unidades_raw = unidades_raw[offset:]
        
        # Filtro post-consulta para b√∫squeda parcial en nombre
        if nombre_up:
            def matches_name(unidad: Dict) -> bool:
                properties = unidad.get('properties', {})
                nombre_campo = properties.get('nombre_up', '') or properties.get('nombre', '')
                return nombre_up.lower() in str(nombre_campo).lower()
            
            unidades_raw = list(filter(matches_name, unidades_raw))
            filters_applied['nombre_up'] = f"contains '{nombre_up}'"
        
        # Procesamiento final con metadatos opcionales
        if include_metadata:
            # Agregar metadatos usando map funcional
            unidades_processed = [
                {
                    **unidad, 
                    '_metadata': {
                        'create_time': getattr(unidad.get('_doc_ref'), 'create_time', None),
                        'update_time': getattr(unidad.get('_doc_ref'), 'update_time', None)
                    }
                } 
                for unidad in unidades_raw
            ]
        else:
            unidades_processed = unidades_raw
        
        # Limpiar referencias de documentos
        for unidad in unidades_processed:
            unidad.pop('_doc_ref', None)
        
        return unidades_processed, filters_applied
    
    success, result, error = await execute_firestore_query(build_optimized_query)
    
    if not success:
        return {
            "success": False,
            "error": error,
            "data": [],
            "count": 0,
            "filters_applied": {},
            "pagination": {}
        }
    
    unidades, filters_applied = result
    
    # Informaci√≥n de paginaci√≥n
    pagination_info = {
        "limit": limit,
        "offset": offset or 0,
        "returned_count": len(unidades),
        "has_more": len(unidades) == limit if limit else False
    }
    
    return {
        "success": True,
        "data": unidades,
        "count": len(unidades),
        "filters_applied": filters_applied,
        "pagination": pagination_info,
        "timestamp": datetime.now().isoformat(),
        "collection": "unidades_proyecto",
        "cached": True,
        "optimizations": {
            "batch_queries": True,
            "functional_filtering": True,
            "pagination_supported": True,
            "post_query_text_search": bool(nombre_up)
        }
    }



@cache_result(ttl=900)  # 15 minutos de cach√© para dashboard
async def get_dashboard_summary() -> Dict[str, Any]:
    """
    Obtener resumen ejecutivo extremadamente optimizado para dashboards
    
    Returns:
        Dict con m√©tricas y distribuciones calculadas funcionalmente
    """
    try:
        # Obtener datos sin metadatos para optimizaci√≥n
        result = await get_all_unidades_proyecto(include_metadata=False)
        
        if not result["success"]:
            return result
        
        unidades = result["data"]
        
        # Calcular m√©tricas usando programaci√≥n funcional
        estadisticas = calculate_statistics(unidades)
        
        # Calcular m√©tricas adicionales espec√≠ficas para dashboard
        dashboard_metrics = _calculate_dashboard_metrics(unidades)
        
        # Combinar estad√≠sticas usando pipe
        combined_stats = pipe(
            estadisticas,
            lambda stats: {**stats, **dashboard_metrics},
            lambda merged: {
                **merged,
                "performance_indicators": _calculate_kpis(unidades),
                "geographic_distribution": _calculate_geographic_stats(unidades)
            }
        )
        
        return {
            "success": True,
            **combined_stats,
            "timestamp": datetime.now().isoformat(),
            "collection": "unidades_proyecto",
            "cached": True,
            "optimization": {
                "functional_programming": True,
                "memory_optimized": True,
                "computation_time_ms": "< 100ms (cached)"
            }
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"Error generando resumen de dashboard: {str(e)}",
            "metrics": {},
            "distribuciones": {}
        }

def _calculate_dashboard_metrics(unidades: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calcular m√©tricas espec√≠ficas para dashboard de forma funcional
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Dict con m√©tricas adicionales para dashboard
    """
    if not unidades:
        return {"metrics": {}, "trends": {}}
    
    # Extraer datos con funciones puras
    extract_ano = lambda u: safe_get(u, 'properties.ano')
    extract_estado = lambda u: safe_get(u, 'properties.estado')
    
    # Calcular tendencias por a√±o
    anos_data = [ano for ano in map(extract_ano, unidades) if ano]
    trend_by_year = group_by(lambda x: x, anos_data)
    
    # Calcular distribuci√≥n de estados
    estados_data = [estado for estado in map(extract_estado, unidades) if estado]
    estado_distribution = group_by(lambda x: x, estados_data)
    
    # M√©tricas de calidad
    total_unidades = len(unidades)
    con_coordenadas = sum(1 for u in unidades if safe_get(u, 'geometry.coordinates'))
    con_bpin = sum(1 for u in unidades if safe_get(u, 'properties.bpin'))
    
    return {
        "metrics": {
            "calidad_datos": {
                "con_coordenadas": con_coordenadas,
                "porcentaje_coordenadas": (con_coordenadas / total_unidades * 100) if total_unidades else 0,
                "con_bpin": con_bpin,
                "porcentaje_bpin": (con_bpin / total_unidades * 100) if total_unidades else 0
            }
        },
        "trends": {
            "por_ano": {ano: len(items) for ano, items in trend_by_year.items()},
            "estados_activos": {estado: len(items) for estado, items in estado_distribution.items()}
        }
    }

def _calculate_kpis(unidades: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calcular indicadores clave de rendimiento (KPIs)
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Dict con KPIs calculados
    """
    if not unidades:
        return {}
    
    total = len(unidades)
    
    # KPIs funcionales
    kpis = {
        "total_proyectos": total,
        "cobertura_geografica": len(set(safe_get(u, 'properties.comuna_corregimiento') for u in unidades if safe_get(u, 'properties.comuna_corregimiento'))),
        "diversidad_fuentes": len(set(safe_get(u, 'properties.fuente_financiacion') for u in unidades if safe_get(u, 'properties.fuente_financiacion'))),
        "tipos_intervencion": len(set(safe_get(u, 'properties.tipo_intervencion') for u in unidades if safe_get(u, 'properties.tipo_intervencion')))
    }
    
    return kpis

def _calculate_geographic_stats(unidades: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Calcular estad√≠sticas geogr√°ficas
    
    Args:
        unidades: Lista de unidades de proyecto
        
    Returns:
        Dict con estad√≠sticas geogr√°ficas
    """
    if not unidades:
        return {}
    
    # Extraer coordenadas v√°lidas (con valores num√©ricos no None)
    coordenadas = []
    for u in unidades:
        coords = safe_get(u, 'geometry.coordinates')
        if (coords and 
            len(coords) >= 2 and 
            coords[0] is not None and 
            coords[1] is not None and
            isinstance(coords[0], (int, float)) and 
            isinstance(coords[1], (int, float))):
            coordenadas.append(coords)
    
    if not coordenadas:
        return {"coverage": "sin_datos"}
    
    # Calcular bounding box con coordenadas v√°lidas
    longitudes = [coord[0] for coord in coordenadas]
    latitudes = [coord[1] for coord in coordenadas]
    
    return {
        "total_con_coordenadas": len(coordenadas),
        "bounding_box": {
            "min_longitude": min(longitudes),
            "max_longitude": max(longitudes),
            "min_latitude": min(latitudes),
            "max_latitude": max(latitudes)
        },
        "center_point": {
            "longitude": sum(longitudes) / len(longitudes),
            "latitude": sum(latitudes) / len(latitudes)
        }
    }

# ============================================================================
# FUNCIONES DE ELIMINACI√ìN OPTIMIZADAS
# ============================================================================

async def delete_all_unidades_proyecto() -> Dict[str, Any]:
    """
    Eliminar todas las unidades de proyecto de la colecci√≥n
    
    PRECAUCI√ìN: Esta operaci√≥n eliminar√° TODOS los documentos de la colecci√≥n
    
    Returns:
        Dict con el resultado de la operaci√≥n de eliminaci√≥n
    """
    def delete_all_documents(db: firestore.Client) -> Dict[str, Any]:
        collection_ref = db.collection('unidades_proyecto')
        
        # Obtener todos los documentos para eliminar
        docs = collection_ref.stream()
        
        deleted_count = 0
        batch_size = 50  # Firestore batch limit
        batch = db.batch()
        current_batch_count = 0
        
        for doc in docs:
            batch.delete(doc.reference)
            current_batch_count += 1
            deleted_count += 1
            
            # Ejecutar batch cuando alcance el l√≠mite
            if current_batch_count >= batch_size:
                batch.commit()
                batch = db.batch()  # Nuevo batch
                current_batch_count = 0
        
        # Ejecutar batch final si hay documentos pendientes
        if current_batch_count > 0:
            batch.commit()
        
        return {
            "deleted_count": deleted_count,
            "operation": "delete_all"
        }
    
    success, result, error = await execute_firestore_query(delete_all_documents)
    
    if not success:
        return {
            "success": False,
            "error": error,
            "deleted_count": 0
        }
    
    # Limpiar cach√© despu√©s de eliminaci√≥n masiva
    await cache.clear()
    
    return {
        "success": True,
        "message": "Todos los documentos de unidades_proyecto han sido eliminados",
        **result,
        "timestamp": datetime.now().isoformat(),
        "cache_cleared": True
    }

async def delete_unidades_proyecto_by_criteria(
    upid: Optional[str] = None,
    bpin: Optional[str] = None,
    referencia_proceso: Optional[str] = None,
    referencia_contrato: Optional[str] = None,
    fuente_financiacion: Optional[str] = None,
    tipo_intervencion: Optional[str] = None
) -> Dict[str, Any]:
    """
    Eliminar unidades de proyecto por criterios espec√≠ficos
    
    Args:
        upid: Eliminar por UPID espec√≠fico
        bpin: Eliminar por BPIN espec√≠fico
        referencia_proceso: Eliminar por referencia de proceso
        referencia_contrato: Eliminar por referencia de contrato
        fuente_financiacion: Eliminar por fuente de financiaci√≥n
        tipo_intervencion: Eliminar por tipo de intervenci√≥n
    
    Returns:
        Dict con el resultado de la operaci√≥n de eliminaci√≥n
    """
    # Validar que al menos un criterio est√© presente
    criterios = {
        'upid': upid,
        'bpin': bpin,
        'referencia_proceso': referencia_proceso,
        'referencia_contrato': referencia_contrato,
        'fuente_financiacion': fuente_financiacion,
        'tipo_intervencion': tipo_intervencion
    }
    
    criterios_validos = {k: v for k, v in criterios.items() if v is not None}
    
    if not criterios_validos:
        return {
            "success": False,
            "error": "Debe proporcionar al menos un criterio de eliminaci√≥n",
            "deleted_count": 0
        }
    
    def delete_by_criteria(db: firestore.Client) -> Dict[str, Any]:
        collection_ref = db.collection('unidades_proyecto')
        query = collection_ref
        
        # Construir consulta con filtros
        for campo, valor in criterios_validos.items():
            query = query.where(f'properties.{campo}', '==', valor)
        
        # Obtener documentos que coinciden con los criterios
        docs = list(query.stream())
        
        if not docs:
            return {
                "deleted_count": 0,
                "matched_documents": 0,
                "criteria": criterios_validos
            }
        
        # Eliminar documentos en lotes
        deleted_count = 0
        batch_size = 50
        batch = db.batch()
        current_batch_count = 0
        
        for doc in docs:
            batch.delete(doc.reference)
            current_batch_count += 1
            deleted_count += 1
            
            if current_batch_count >= batch_size:
                batch.commit()
                batch = db.batch()
                current_batch_count = 0
        
        # Ejecutar batch final
        if current_batch_count > 0:
            batch.commit()
        
        return {
            "deleted_count": deleted_count,
            "matched_documents": len(docs),
            "criteria": criterios_validos
        }
    
    success, result, error = await execute_firestore_query(delete_by_criteria)
    
    if not success:
        return {
            "success": False,
            "error": error,
            "deleted_count": 0
        }
    
    # Invalidar cach√© parcialmente basado en los criterios
    await _invalidate_cache_by_criteria(criterios_validos)
    
    return {
        "success": True,
        "message": f"Eliminados {result['deleted_count']} documentos que coinciden con los criterios",
        **result,
        "timestamp": datetime.now().isoformat(),
        "cache_invalidated": True
    }

async def _invalidate_cache_by_criteria(criterios: Dict[str, Any]) -> None:
    """
    Invalidar cach√© de forma inteligente basada en criterios de eliminaci√≥n
    
    Args:
        criterios: Criterios de eliminaci√≥n que afectan el cach√©
    """
    # Para simplificar, limpiaremos todo el cach√©
    # En una implementaci√≥n m√°s sofisticada, podr√≠amos invalidar selectivamente
    await cache.clear()

# ============================================================================
# FUNCIONES DE PAGINACI√ìN AVANZADA
# ============================================================================

async def get_unidades_proyecto_paginated(
    page: int = 1,
    page_size: int = 50,
    filters: Optional[Dict[str, Any]] = None,
    order_by: Optional[str] = None,
    order_direction: str = 'asc'
) -> Dict[str, Any]:
    """
    Obtener unidades de proyecto con paginaci√≥n avanzada
    
    Args:
        page: N√∫mero de p√°gina (inicia en 1)
        page_size: Tama√±o de p√°gina (m√°ximo 100)
        filters: Filtros a aplicar
        order_by: Campo para ordenar
        order_direction: Direcci√≥n de ordenamiento ('asc' o 'desc')
    
    Returns:
        Dict con datos paginados y metadatos de paginaci√≥n
    """
    # Validar par√°metros
    page = max(1, page)
    page_size = min(100, max(1, page_size))
    offset = (page - 1) * page_size
    
    # Aplicar filtros si existen
    if filters:
        result = await filter_unidades_proyecto(
            limit=page_size,
            offset=offset,
            **filters
        )
    else:
        result = await get_all_unidades_proyecto(
            limit=page_size
        )
        # Aplicar offset manualmente para get_all
        if offset > 0 and result["success"]:
            result["data"] = result["data"][offset:offset + page_size]
    
    if not result["success"]:
        return result
    
    # Agregar metadatos de paginaci√≥n
    total_count = result.get("count", 0)
    has_next = len(result["data"]) == page_size
    has_prev = page > 1
    
    pagination_meta = {
        "page": page,
        "page_size": page_size,
        "total_returned": len(result["data"]),
        "has_next_page": has_next,
        "has_prev_page": has_prev,
        "next_page": page + 1 if has_next else None,
        "prev_page": page - 1 if has_prev else None
    }
    
    return {
        **result,
        "pagination": pagination_meta,
        "paginated": True
    }